{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assembly</th>\n",
       "      <th>Material</th>\n",
       "      <th>OpenOrders</th>\n",
       "      <th>NewOrders</th>\n",
       "      <th>TotalWork</th>\n",
       "      <th>TotalSetup</th>\n",
       "      <th>SumDuration</th>\n",
       "      <th>SumOperations</th>\n",
       "      <th>ProductionOrders</th>\n",
       "      <th>CycleTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17626.000000</td>\n",
       "      <td>1.762600e+04</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.00000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84.995723</td>\n",
       "      <td>7.582117e+08</td>\n",
       "      <td>35.159197</td>\n",
       "      <td>3.951265</td>\n",
       "      <td>8.807599</td>\n",
       "      <td>10.314143</td>\n",
       "      <td>220.348746</td>\n",
       "      <td>21.83791</td>\n",
       "      <td>4.367582</td>\n",
       "      <td>4440.145013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.860778</td>\n",
       "      <td>1.582321e+08</td>\n",
       "      <td>16.162640</td>\n",
       "      <td>1.927908</td>\n",
       "      <td>1.017044</td>\n",
       "      <td>0.850116</td>\n",
       "      <td>87.248204</td>\n",
       "      <td>8.70554</td>\n",
       "      <td>1.741108</td>\n",
       "      <td>2186.389034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.573511</td>\n",
       "      <td>1.921125e+08</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>77.667220</td>\n",
       "      <td>7.000179e+08</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.270000</td>\n",
       "      <td>9.730000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2802.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>92.715866</td>\n",
       "      <td>8.244681e+08</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3790.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>97.449125</td>\n",
       "      <td>8.703534e+08</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.480000</td>\n",
       "      <td>10.850000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5600.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.246370</td>\n",
       "      <td>9.245294e+08</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.480000</td>\n",
       "      <td>13.280000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14855.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Assembly      Material    OpenOrders     NewOrders     TotalWork  \\\n",
       "count  17626.000000  1.762600e+04  17626.000000  17626.000000  17626.000000   \n",
       "mean      84.995723  7.582117e+08     35.159197      3.951265      8.807599   \n",
       "std       17.860778  1.582321e+08     16.162640      1.927908      1.017044   \n",
       "min       20.573511  1.921125e+08      7.000000      0.000000      2.790000   \n",
       "25%       77.667220  7.000179e+08     24.000000      3.000000      8.270000   \n",
       "50%       92.715866  8.244681e+08     30.000000      4.000000      8.950000   \n",
       "75%       97.449125  8.703534e+08     42.000000      5.000000      9.480000   \n",
       "max      104.246370  9.245294e+08     91.000000     12.000000     11.480000   \n",
       "\n",
       "         TotalSetup   SumDuration  SumOperations  ProductionOrders  \\\n",
       "count  17626.000000  17626.000000    17626.00000      17626.000000   \n",
       "mean      10.314143    220.348746       21.83791          4.367582   \n",
       "std        0.850116     87.248204        8.70554          1.741108   \n",
       "min        7.230000    145.000000       15.00000          3.000000   \n",
       "25%        9.730000    153.000000       15.00000          3.000000   \n",
       "50%       10.260000    197.000000       20.00000          4.000000   \n",
       "75%       10.850000    255.000000       25.00000          5.000000   \n",
       "max       13.280000    442.000000       45.00000          9.000000   \n",
       "\n",
       "          CycleTime  \n",
       "count  17626.000000  \n",
       "mean    4440.145013  \n",
       "std     2186.389034  \n",
       "min     1475.000000  \n",
       "25%     2802.000000  \n",
       "50%     3790.500000  \n",
       "75%     5600.750000  \n",
       "max    14855.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import sklearn\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "#from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "#set_random_seed(1)\n",
    "seed(1)\n",
    "\n",
    "train = pd.read_csv('TestData/train.csv',',')\n",
    "predTest = pd.read_csv('TestData/testData1.csv',';')\n",
    "#train = pd.read_csv('TestData/TrainingData_8Weeks.csv',';' , parse_dates=['Time'])\n",
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assembly</th>\n",
       "      <th>Material</th>\n",
       "      <th>OpenOrders</th>\n",
       "      <th>NewOrders</th>\n",
       "      <th>TotalWork</th>\n",
       "      <th>TotalSetup</th>\n",
       "      <th>SumDuration</th>\n",
       "      <th>SumOperations</th>\n",
       "      <th>ProductionOrders</th>\n",
       "      <th>CycleTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>145</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>145</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>147</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17621</th>\n",
       "      <td>101.64704</td>\n",
       "      <td>888701300</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5.88</td>\n",
       "      <td>11.96</td>\n",
       "      <td>150</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>101.26033</td>\n",
       "      <td>897286900</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.30</td>\n",
       "      <td>11.62</td>\n",
       "      <td>156</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17623</th>\n",
       "      <td>101.26033</td>\n",
       "      <td>897286900</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.30</td>\n",
       "      <td>11.62</td>\n",
       "      <td>197</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17624</th>\n",
       "      <td>101.26033</td>\n",
       "      <td>897286900</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.30</td>\n",
       "      <td>11.62</td>\n",
       "      <td>255</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>101.78664</td>\n",
       "      <td>900852860</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6.41</td>\n",
       "      <td>11.75</td>\n",
       "      <td>156</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17626 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Assembly   Material  OpenOrders  NewOrders  TotalWork  TotalSetup  \\\n",
       "0       25.31369  284485100          24          3       8.50       10.36   \n",
       "1       25.31369  284485100          24          3       8.50       10.36   \n",
       "2       25.31369  284485100          24          3       8.50       10.36   \n",
       "3       25.31369  284485100          24          3       8.50       10.36   \n",
       "4       25.31369  284485100          24          3       8.50       10.36   \n",
       "...          ...        ...         ...        ...        ...         ...   \n",
       "17621  101.64704  888701300          16          2       5.88       11.96   \n",
       "17622  101.26033  897286900          19          4       6.30       11.62   \n",
       "17623  101.26033  897286900          19          4       6.30       11.62   \n",
       "17624  101.26033  897286900          19          4       6.30       11.62   \n",
       "17625  101.78664  900852860          20          4       6.41       11.75   \n",
       "\n",
       "       SumDuration  SumOperations  ProductionOrders  CycleTime  \n",
       "0              145             15                 3       1808  \n",
       "1              145             15                 3       1525  \n",
       "2              200             20                 4       2735  \n",
       "3              200             20                 4       2248  \n",
       "4              147             15                 3       2056  \n",
       "...            ...            ...               ...        ...  \n",
       "17621          150             15                 3       1860  \n",
       "17622          156             15                 3       2284  \n",
       "17623          197             20                 4       2026  \n",
       "17624          255             25                 5       2555  \n",
       "17625          156             15                 3       1839  \n",
       "\n",
       "[17626 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.70\n",
    "train_split = int(split_fraction * int(train.shape[0]))\n",
    "step = 1\n",
    "\n",
    "#past = 0\n",
    "#future = 8\n",
    "learning_rate = 0.0005\n",
    "batch = 256\n",
    "epochs = 100\n",
    "mean = 0\n",
    "std = 0\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    global mean\n",
    "    global std\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    mean = data_mean\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    std = data_std\n",
    "    return (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.857361</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.243078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.857361</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.362727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.228713</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.851154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.228713</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-1.057052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.834501</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.138227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4        5         6  \\\n",
       "0 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.857361   \n",
       "1 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.857361   \n",
       "2 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.228713   \n",
       "3 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.228713   \n",
       "4 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.834501   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.779658 -0.779658 -1.243078  \n",
       "1 -0.779658 -0.779658 -1.362727  \n",
       "2 -0.207005 -0.207005 -0.851154  \n",
       "3 -0.207005 -0.207005 -1.057052  \n",
       "4 -0.779658 -0.779658 -1.138227  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "for c in train.columns:\n",
    "    titles.append(c);\n",
    "    \n",
    "features = train[titles]\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>-0.857361</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.243078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>-0.857361</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.362727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>-0.228713</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.851154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>-0.228713</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-1.057052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>-0.834501</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.138227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>-0.596206</td>\n",
       "      <td>-0.807596</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>0.612441</td>\n",
       "      <td>-0.585867</td>\n",
       "      <td>-0.240143</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.438090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>-0.596206</td>\n",
       "      <td>-0.807596</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>0.612441</td>\n",
       "      <td>-0.585867</td>\n",
       "      <td>-0.731632</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.175009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>-0.451691</td>\n",
       "      <td>-0.769843</td>\n",
       "      <td>-0.893266</td>\n",
       "      <td>-0.011827</td>\n",
       "      <td>0.523635</td>\n",
       "      <td>-1.050269</td>\n",
       "      <td>-0.731632</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.977567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>-0.451691</td>\n",
       "      <td>-0.769843</td>\n",
       "      <td>-0.893266</td>\n",
       "      <td>-0.011827</td>\n",
       "      <td>0.523635</td>\n",
       "      <td>-1.050269</td>\n",
       "      <td>2.251589</td>\n",
       "      <td>2.083605</td>\n",
       "      <td>2.083605</td>\n",
       "      <td>0.471750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>-0.421817</td>\n",
       "      <td>-0.807672</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.331029</td>\n",
       "      <td>-0.240143</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.682039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12338 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.116690 -0.857361   \n",
       "1     -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.116690 -0.857361   \n",
       "2     -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.116690 -0.228713   \n",
       "3     -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.116690 -0.228713   \n",
       "4     -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.116690 -0.834501   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "12333 -0.596206 -0.807596 -0.778773 -0.532953  0.612441 -0.585867 -0.240143   \n",
       "12334 -0.596206 -0.807596 -0.778773 -0.532953  0.612441 -0.585867 -0.731632   \n",
       "12335 -0.451691 -0.769843 -0.893266 -0.011827  0.523635 -1.050269 -0.731632   \n",
       "12336 -0.451691 -0.769843 -0.893266 -0.011827  0.523635 -1.050269  2.251589   \n",
       "12337 -0.421817 -0.807672 -0.950513 -1.054079  0.267086  0.331029 -0.240143   \n",
       "\n",
       "              7         8         9  \n",
       "0     -0.779658 -0.779658 -1.243078  \n",
       "1     -0.779658 -0.779658 -1.362727  \n",
       "2     -0.207005 -0.207005 -0.851154  \n",
       "3     -0.207005 -0.207005 -1.057052  \n",
       "4     -0.779658 -0.779658 -1.138227  \n",
       "...         ...       ...       ...  \n",
       "12333 -0.207005 -0.207005 -0.438090  \n",
       "12334 -0.779658 -0.779658 -1.175009  \n",
       "12335 -0.779658 -0.779658 -0.977567  \n",
       "12336  2.083605  2.083605  0.471750  \n",
       "12337 -0.207005 -0.207005 -0.682039  \n",
       "\n",
       "[12338 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = features.loc[0 : train_split - 1] #Training Data\n",
    "val_data = features.loc[train_split:] #Validation Data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.21256498, -2.87541672, -0.77877323, -0.53295299, -0.34468528,\n",
       "        0.11668968, -0.85736119, -0.77965774, -0.77965774])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start = past + future\n",
    "start = 0\n",
    "end = start + train_split\n",
    "\n",
    "x_train = train_data[[i for i in range(9)]].values\n",
    "y_train = features.iloc[start:end][[9]]\n",
    "\n",
    "#sequence_length = int(past / step)\n",
    "sequence_length = 1\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (256, 1, 9)\n",
      "Target shape: (256, 1)\n"
     ]
    }
   ],
   "source": [
    "label_start = train_split\n",
    "valRange = int(train.shape[0]) - train_split\n",
    "\n",
    "# x_val = val_data.iloc[[i for i in range(valRange)]].values\n",
    "x_val = val_data[[i for i in range(9)]].values\n",
    "# x_val = val_data.iloc[[i for i in range(49)]].values\n",
    "y_val = features.iloc[label_start:][[9]]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    \n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 9)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1, 10)             100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 111\n",
      "Trainable params: 111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# Long Short Term Memory - Model als Methodik mit Adam --> stochastic gradient descent algorithm\n",
    "\n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "# lstm_out = keras.layers.LSTM(10)(inputs)\n",
    "dense1 = keras.layers.Dense(10, activation=\"tanh\")(inputs)\n",
    "outputs = keras.layers.Dense(1)(dense1)\n",
    "learning_rate = 0.001\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#model.save(\"kerasModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/4000\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1132\n",
      "Epoch 00001: val_loss improved from inf to 0.08382, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.1105 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 2/4000\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00002: val_loss improved from 0.08382 to 0.08375, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 3/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00003: val_loss improved from 0.08375 to 0.08370, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 4/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00004: val_loss improved from 0.08370 to 0.08367, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1104 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 5/4000\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00005: val_loss improved from 0.08367 to 0.08364, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1104 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 6/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00006: val_loss improved from 0.08364 to 0.08362, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 7/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00007: val_loss improved from 0.08362 to 0.08360, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 8/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00008: val_loss improved from 0.08360 to 0.08359, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 9/4000\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00009: val_loss improved from 0.08359 to 0.08357, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 10/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00010: val_loss improved from 0.08357 to 0.08356, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 11/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00011: val_loss improved from 0.08356 to 0.08356, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 12/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00012: val_loss improved from 0.08356 to 0.08355, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 13/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00013: val_loss improved from 0.08355 to 0.08354, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 14/4000\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00014: val_loss improved from 0.08354 to 0.08354, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 15/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00015: val_loss improved from 0.08354 to 0.08353, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 16/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00016: val_loss improved from 0.08353 to 0.08353, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 17/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00017: val_loss improved from 0.08353 to 0.08352, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 18/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00018: val_loss improved from 0.08352 to 0.08352, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 19/4000\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00019: val_loss improved from 0.08352 to 0.08352, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 20/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00020: val_loss improved from 0.08352 to 0.08351, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 21/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00021: val_loss improved from 0.08351 to 0.08351, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 22/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00022: val_loss improved from 0.08351 to 0.08351, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 23/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00023: val_loss improved from 0.08351 to 0.08351, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 24/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00024: val_loss improved from 0.08351 to 0.08351, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 25/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00025: val_loss improved from 0.08351 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 26/4000\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00026: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 27/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00027: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 28/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00028: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 29/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00029: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1103 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 30/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00030: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 31/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00031: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 32/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00032: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 33/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00033: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 34/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00034: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 35/4000\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00035: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 36/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00036: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 37/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00037: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 38/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00038: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 39/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00039: val_loss improved from 0.08350 to 0.08350, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 40/4000\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00040: val_loss did not improve from 0.08350\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 41/4000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00041: val_loss did not improve from 0.08350\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 42/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00042: val_loss did not improve from 0.08350\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 43/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00043: val_loss did not improve from 0.08350\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 44/4000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00044: val_loss did not improve from 0.08350\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1102 - val_loss: 0.0835\n"
     ]
    }
   ],
   "source": [
    "path_checkpoint = \"simpleModelCheckpoint.h5\"\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 0.0001\n",
    "    if lr > 0.004:\n",
    "        return lr - 0.0002\n",
    "    else:\n",
    "        if lr > 0.0004:\n",
    "            return lr - 0.000001\n",
    "        else:            \n",
    "            return 0.0001\n",
    "#     if epoch < 10:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         if lr < 0.002:\n",
    "#             return lr - 0.00001\n",
    "#         else:\n",
    "#             return lr - 0.0015\n",
    "#     if lr < 0.002:\n",
    "#         return lr - 0.00001\n",
    "#     else:\n",
    "#         return lr\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback,modelckpt_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_loss(history, \"Training and Validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAj4UlEQVR4nO3dfZhVdb3+8fcNjKA8hAoaioEaoabDgKiIDzUhFKRg+YAeRMwKPR4zTU9hD4hmytEM9dRPLjOVhJQkzWcPQpBahoKMo0WGJgiCgCAKCjHA5/fHXkybYdY87ZnZM8P9uq659l5rfddanzUDc8/6rr2+SxGBmZlZZVrluwAzM2u6HBJmZpbKIWFmZqkcEmZmlsohYWZmqRwSZmaWyiFhjUbSKEkz62lbcyV9oz621VJkf09y+V5LekrSmPqtzporh4TVK0knSvqzpA8krZP0J0nHAETEtIgY0gRq/Lqkv0vaIGmVpCckdUyW3Svp+gbcd0j6SNJGSe9I+pmk1vW9n5p+ryVNkDS1wrpDI2JKfddkzVObfBdgLYekTsDjwH8CvwX2AE4C/pXPurJJ+hxwA/CliFgoaR/gtEYuo09EvCHpMGAu8A9gcoU620TE1kauy2wXPpOw+vQZgIi4PyK2RcSmiJgZEaUAki6Q9PyOxslf1RdLWizpfUm/kKRkWWtJt0h6T9Jbki5N2lf6h42kCyUtSrbzf5J6pNR4DPBCRCxMal0XEVMiYoOkscAo4LvJX/qPJds+QNLvJK1Jarksa78TJM2QND05M3lZUp+afLMi4u/Ac8CRknomx/d1SW8Df6juuCQNTs6IPpD0c0BZyyp+rz8r6Znk7G6VpO9L+hLwfWBkcryvJG2zu61aSfqhpKWSVkv6taRPJMt21DxG0tvJz+oHNTl2az4cElaf/gFskzRF0lBJe9dgnVPJ/OLuA5wNfDGZ/01gKFAE9ANOT9uApNPJ/LL7KtCVzC/e+1OazwO+KOlaSSdIartjQUTcCUwDboqIDhFxmqRWwGPAK8CBwCDgcklfzNrmCOBBYB/gN8DvJRVUd+CSjiBzprUwa/bngMOTGlOPS1IX4HfAD4EuwJvACSn76QjMAp4GDgA+DcyOiKfJnFVNT463snC7IPkqBg4BOgA/r9DmRKA3me/NeEmHV3fs1nw4JKzeRMSHZH5hBPBLYI2kRyXtX8VqEyNifUS8DcwhEwqQCYzbImJ5RLwPTKxiGxcBN0bEoqSL5gagqLKziYh4jswv3X7AE8Daaq4LHAN0jYjrImJLRPwzObZzstosiIgZEVEG/AxoBwyoot6XJb1PJnzuAu7JWjYhIj6KiE3VHNcw4G9Z+70VeDdlf6cC70bELRGxOSI2RMS8KurLNgr4WUT8MyI2AlcD51Q4o7s2OWt8hUyY1uhMypoHh4TVq+QX2gUR0R04ksxfrrdWsUr2L7aPyfylSrLesqxl2e8r6gHcJmm9pPXAOjJdLwem1PhURJxG5i//EWT+Uk77pFQP4IAd2062/30gO/jKa4uI7cDypP40/SJi74g4NCJ+mKyzy7aqOa6dvj+RGakz7Xt0EJkzjbo4AFiaNb2UzLXM7ONP+xlaC+CQsAaT9LnfSyYsamsl0D1r+qAq2i4DLoqIzllfe0bEn6upb3tEzCbT/7+jxorDIi8D3qqw7Y4RMayy2pLuqe7AiqoPL72sGh7Xygr7Fenfo2XAoTXYX2VWkAmrHT4FbAVWVbOetRAOCas3kg6TdKWk7sn0QcC5wF/qsLnfAt+WdKCkzsD3qmg7Gbha0meT/X5C0lkpNY6QdI6kvZVxLJnrADtqXEWm732HF4EPJX1P0p7JBfUjlXysN3G0pK8mXTCXk/k0V12OuTbH9QTw2az9XgZ8MmU7jwOflHS5pLaSOko6Lut4eybhVpn7gSskHSypA/++huFPXu0mHBJWnzYAxwHzJH1E5hfla8CVddjWL4GZQCmZC7tPkvkLdlvFhhHxMPA/wAOSPkz2OTRlu++TuSi+GPgQmArcHBHTkuW/Ao5Iunh+HxHbyHxEtgh4C3iPzHWET2Rt8xFgZLLt0cBXk+sEOanquCLiPeAsMtdq1gK9gD+lbGcDMDg5jneTYy9OFj+YvK6V9HIlq98N3Ac8S+b4NwPfyvXYrPmQHzpkzYGkocDkiEj7aGteSJoAfDoizst3LWYNwWcS1iQlXTvDJLWRdCBwDfBwvusy2904JKypEnAtmS6chcAiYHxeKzLbDbm7yczMUvlMwszMUjXLAf66dOkSPXv2zHcZZmbNyoIFC96LiK61WadZhkTPnj2ZP39+vsswM2tWJC2tvtXO3N1kZmapHBJmZpbKIWFmZqma5TUJM6sfZWVlLF++nM2bN+e7FKtH7dq1o3v37hQUVPtYk2o5JMx2Y8uXL6djx4707NmTzECy1txFBGvXrmX58uUcfPDBOW/P3U1mu7HNmzez7777OiBaEEnsu+++9XZ22CxDYvWGf7Fg6fv5LsOsRXBAtDz1+TNtliGx6sPNjLrrLw4KM7MG1ixDAqBs63b+8s+1+S7DzHLUocPOTzu99957ufTSSwGYPHkyv/71r1PXnTt3Ln/+c5UPILQcNdsL1wVtWjHgkH3zXYaZNaCLL764yuVz586lQ4cODBw4sMbb3Lp1K23aNNtffY2uWZ5J7N+pHdO+MYCje+yd71LMdjsLlr7PL+a80SjdvRMmTOCnP/0pALfffjtHHHEEhYWFnHPOOSxZsoTJkyczadIkioqKeO6551i6dCmDBg2isLCQQYMG8fbbbwNwwQUX8J3vfIfi4mL++7//m169erFmzRoAtm/fzqc//Wnee++9Bj+e5qhZxul+Hds6IMzq2bWP/ZW/rfiwyjYbNpfx93c3sD2gleCwT3akY7v0z+IfcUAnrjnts1Vuc9OmTRQVFZVPr1u3juHDh+/SbuLEibz11lu0bduW9evX07lzZy6++GI6dOjAVVddBcBpp53G+eefz5gxY7j77ru57LLL+P3vfw/AP/7xD2bNmkXr1q3p3Lkz06ZN4/LLL2fWrFn06dOHLl26VFnn7qpZnkmYWX58uHkr25NH0GyPzHSu9txzT0pKSsq/rrvuukrbFRYWMmrUKKZOnZraXfTCCy/wH//xHwCMHj2a559/vnzZWWedRevWrQG48MILy6913H333Xzta1/L+ThaqmZ5JmFm9a+6v/gh09U06q6/ULZ1OwVtWnHbOX0b7az+iSee4Nlnn+XRRx/lxz/+MX/961+rXSf7o6Dt27cvf3/QQQex//7784c//IF58+Yxbdq0Bqm5JfCZRGO66SaYM6fqNnPmZNqZNUFH99ibad8YwHeG9G7U64Lbt29n2bJlFBcXc9NNN7F+/Xo2btxIx44d2bBhQ3m7gQMH8sADDwAwbdo0TjzxxNRtfuMb3+C8887j7LPPLj/DsF05JBrTMcfA2WenB8WcOZnlxxzTuHWZ1cLRPfbmv4o/3ajXBbdt28Z5553HUUcdRd++fbniiivo3Lkzp512Gg8//HD5hevbb7+de+65h8LCQu677z5uu+221G0OHz6cjRs3uqupGs3yGdf9+/ePZvvQoR1B8NvfQnFx9fPNGtCiRYs4/PDD811GXsyfP58rrriC5557Lt+lNIjKfraSFkRE/9psx2cSja24OBME2WcUDgizRjVx4kTOOOMMbrzxxnyX0uQ5JPIhOyjGj3dAmDWycePGsXTp0iqvWViGQyJfiovhP/8TfvzjzKsDwsyaIIdEvsyZA3fcAT/6Uea1uk89mZnlgUMiH7KvQVx33a7XKMzMmgiHRGOr7CJ1ZRezzZoK39+zW3NINKaqPsXkoLCmqgHv71m7di1FRUUUFRXxyU9+kgMPPLB8esuWLTkWnvH5z3+e3r17l293xowZqW2XLFnCb37zm3rZb0vhYTka00svVf0pph1B8dJLvpBtTUf2HzD1fH/PvvvuS0lJCZAZ8TV7sD6ov2G9p02bRv/+1d8esCMkdoz/VFPbtm1rsXdt+0yiMX33u9X/RyouzrQza0oa8f6e7GG9v/e97+00XDjAkUceyZIlSwCYOnUqxx57LEVFRVx00UVs27atxvvIPqPY8eCjcePG8dxzz1FUVMSkSZN2egASwKmnnsrcuXPL1xk/fjzHHXccL7zwQp1raeocEmZWM414f8+OYb1vueWW1DaLFi1i+vTp/OlPf6KkpITWrVunDtQ3atSo8u6mtWvTn2g5ceJETjrpJEpKSrjiiiuqrPGjjz7iyCOPZN68eey77741rqW5cXeTmdVc9v09P/pRg3WLZg/rnWb27NksWLCAY5JrIZs2bWK//fartG1Nu5tqo3Xr1pxxxhm1rqW5ySkkJO0DTAd6AkuAsyOi0sdVSWoNzAfeiYhTk3kTgG8Ca5Jm34+IJ3OpycwaUMX7e4qLGyQosof1btOmDdu3by+f3rx5MwARwZgxY+o0tEb2NiMi9SJ52r4B2rVrVx5kudTS1OXa3TQOmB0RvYDZyXSabwOLKpk/KSKKki8HhFlTlaf7e3r27MnLL78MwMsvv8xbb70FwKBBg5gxYwarV68GMk+0W7p0aY23uWDBAgAeeeQRysrKAHYZerxnz56UlJSUD1X+4osvVrq9XGpp6nINiRHAlOT9FOD0yhpJ6g58Gbgrx/2ZWT7k8f6eM844g3Xr1lFUVMQdd9zBZz7zGQCOOOIIrr/+eoYMGUJhYSGDBw9m5cqVNdrmN7/5Tf74xz9y7LHHMm/evPIzl8LCQtq0aUOfPn2YNGkSJ5xwAgcffDBHHXUUV111Ff369at0e7nU0tTlNFS4pPUR0Tlr+v2I2GWQeUkzgBuBjsBVFbqbLgA+JNMVdWVad1W2Zj1UuFkTUqOhwqv7FJNHMW6SGm2ocEmzJL1WydeImuxA0qnA6ohYUMniO4BDgSJgJZD6UQZJYyXNlzR/zZo1ac3MrL7V5v4ea3GqvXAdEaekLZO0SlK3iFgpqRuwupJmJwDDJQ0D2gGdJE2NiPMiYlXWtn4JPF5FHXcCd0LmTKK6us2sntTkvp0GuoBt+ZfrNYlHgTHJ+zHAIxUbRMTVEdE9InoC5wB/iIjzAJJg2eErwGs51mNmZvUo15CYCAyWtBgYnEwj6QBJNfmk0k2SXpVUChQDVd+9YmZmjSqn+yQiYi0wqJL5K4BhlcyfC8zNmh6dy/7NzKxheVgOM6uRN9e9ySVPXEKnGzvR6tpWdLqxE5c8cQlvrnsz36VZA3JImFm1nlr8FIWTC7nr5bvYsGUDQbBhywbuevkuCicX8tTip+q8bUmMHv3vToWtW7fStWtXTj311Fptp2fPnrz33nt1atOzZ0+OOuoo+vTpw5AhQ3j33Xdrte9s2QMSjh8/nlmzZqW2LSkp4ckn/90z/+ijjzJx4sQ677shOCTMrEpvrnuTMx88k4/LPqZse9lOy8q2l/Fx2cec+eCZdT6jaN++Pa+99hqbNm0C4JlnnuHAAw/Mue7amjNnDq+88gr9+/fnhhtu2GlZROw0PEdNXXfddZxySuoHRHcJieHDhzNuXFUDVzQ+h4SZVemWF26hbFtZlW3KtpUx6S+T6ryPoUOH8sQTTwBw//33c+6555YvW7duHaeffjqFhYUMGDCA0tJSIPPAoiFDhtC3b18uuugism8MzmXY7pNPPpk33niDJUuWcPjhh3PJJZfQr18/li1bxs0338wxxxxDYWEh11xzTfk6P/nJT+jduzennHIKr7/+evn87CHJX3rpJQYOHEifPn049thj+eCDDxg/fjzTp0+nqKiI6dOn7zQ0+dKlSxk0aBCFhYUMGjSIt99+u3ybl112GQMHDuSQQw6p8iFK9cEhYWZVmlo6dZcziIrKtpdxX+l9dd7HOeecwwMPPMDmzZspLS3luOOOK192zTXX0LdvX0pLS7nhhhs4//zzAbj22ms58cQTWbhwIcOHDy//JVqbIcQr8/jjj3PUUUcB8Prrr3P++eezcOFCXn/9dRYvXsyLL75ISUkJCxYs4Nlnn2XBggU88MADLFy4kIceeoiXKrmpcMuWLYwcOZLbbruNV155hVmzZtG+fXuuu+46Ro4cSUlJCSNHjtxpnUsvvZTzzz+f0tJSRo0axWWXXVa+bOXKlTz//PM8/vjjDX7m4aHCzaxKG7dsrNd2lSksLGTJkiXcf//9DBu28wcjn3/+eX73u98B8IUvfIG1a9fywQcf8Oyzz/LQQw8B8OUvf5m9986MCFTXYbuLi4tp3bo1hYWFXH/99axfv54ePXowYMAAAGbOnMnMmTPp27dv5ng3bmTx4sVs2LCBr3zlK+y1115Apsuootdff51u3bqV19SpU6dq63nhhRfKj2/06NF8N+umxtNPP51WrVpxxBFHsGrVqrRN1AuHhJlVqcMeHdiwZUON2uVi+PDhXHXVVcydO3enBwNVNr6cpJ1es9V12O45c+bQpUuX8un169fvNGR5RHD11Vdz0UUX7bTerbfeWmkdFWuqrk11stdv27btTttuSO5uMrMqnVd4HgWtCqpsU9CqgNGFud32dOGFFzJ+/Pjyrp4dTj755PLuorlz59KlSxc6deq00/ynnnqK99/PjA3aUMN2f/GLX+Tuu+9m48bMGdM777zD6tWrOfnkk3n44YfZtGkTGzZs4LHHHttl3cMOO4wVK1aUd0Vt2LCBrVu37jI0ebaBAwfywAMPAJmHJp144ok5H0Nd+EzCzKp05fFXMuWVKVVelyhoXcAVA3IbMKF79+58+9vf3mX+hAkT+NrXvkZhYSF77bUXU6Zknk5wzTXXcO6559KvXz8+97nP8alPfQrYedju7du3U1BQwC9+8Qt69OiRU31Dhgxh0aJFHH/88UDmGddTp06lX79+jBw5kqKiInr06MFJJ520y7p77LEH06dP51vf+habNm1izz33ZNasWRQXFzNx4kSKioq4+uqrd1rn9ttv58ILL+Tmm2+ma9eu3HPPPTnVX1c5DRWeLx4q3Kx+1GiocDL3SZz54JmUbSvbKSwKWhVQ0LqAGWfNYGivoQ1ZqtVSow0VbmY2tNdQSi8uZezRY+nUthOt1IpObTsx9uixlF5c6oBowdzdZGY1cug+h/LzYT/n58N+nu9SrBH5TMJsN9ccu5ytavX5M3VImO3G2rVrx9q1ax0ULUhEsHbtWtq1a1cv23N3k9lurHv37ixfvhw/ErhladeuHd27d6+XbTkkzHZjBQUFHHzwwfkuw5owdzeZmVkqh4SZmaVySJiZWSqHhJmZpXJImJlZKoeEmZmlckiYmVkqh4SZmaVySJiZWSqHhJmZpXJImJlZKoeEmZmlckiYmVkqh4SZmaVySJiZWSqHhJmZpXJImJlZqpxCQtI+kp6RtDh53Tul3RJJr0oqkTS/tuubmVl+5HomMQ6YHRG9gNnJdJriiCiKiP51XN/MzBpZriExApiSvJ8CnN7I65uZWQPKNST2j4iVAMnrfintApgpaYGksXVYH0ljJc2XNH/NmjU5lm1mZjXRproGkmYBn6xk0Q9qsZ8TImKFpP2AZyT9PSKercX6RMSdwJ0A/fv3j9qsa2ZmdVNtSETEKWnLJK2S1C0iVkrqBqxO2caK5HW1pIeBY4FngRqtb2Zm+ZFrd9OjwJjk/RjgkYoNJLWX1HHHe2AI8FpN1zczs/zJNSQmAoMlLQYGJ9NIOkDSk0mb/YHnJb0CvAg8ERFPV7W+mZk1DdV2N1UlItYCgyqZvwIYlrz/J9CnNuubmVnT4DuuzcwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxS5RQSkvaR9Iykxcnr3intlkh6VVKJpPlZ8ydIeieZXyJpWC71mJlZ/cr1TGIcMDsiegGzk+k0xRFRFBH9K8yflMwviognc6zHzMzqUa4hMQKYkryfApye4/bMzKwJyTUk9o+IlQDJ634p7QKYKWmBpLEVll0qqVTS3WndVQCSxkqaL2n+mjVrcizbzMxqotqQkDRL0muVfI2oxX5OiIh+wFDgvySdnMy/AzgUKAJWArekbSAi7oyI/hHRv2vXrrXYtZmZ1VWb6hpExClpyyStktQtIlZK6gasTtnGiuR1taSHgWOBZyNiVda2fgk8XtsDMDOzhpNrd9OjwJjk/RjgkYoNJLWX1HHHe2AI8Foy3S2r6Vd2zDczs6ah2jOJakwEfivp68DbwFkAkg4A7oqIYcD+wMOSduzvNxHxdLL+TZKKyFyzWAJclGM9ZmZWj3IKiYhYCwyqZP4KYFjy/p9An5T1R+eyfzMza1i+49rMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLFVOISFpH0nPSFqcvO6d0q6zpBmS/i5pkaTja7O+mZnlR65nEuOA2RHRC5idTFfmNuDpiDgM6AMsquX6ZmaWB7mGxAhgSvJ+CnB6xQaSOgEnA78CiIgtEbG+puubmVn+5BoS+0fESoDkdb9K2hwCrAHukbRQ0l2S2tdifQAkjZU0X9L8NWvW5Fi2mZnVRLUhIWmWpNcq+RpRw320AfoBd0REX+Aj6tCtFBF3RkT/iOjftWvX2q5uZmZ10Ka6BhFxStoySaskdYuIlZK6AasrabYcWB4R85LpGfw7JGqyvpmZ5Umu3U2PAmOS92OARyo2iIh3gWWSeiezBgF/q+n6ZmaWP7mGxERgsKTFwOBkGkkHSHoyq923gGmSSoEi4Iaq1jczs6ah2u6mqkTEWjJnBhXnrwCGZU2XAP1rur6ZmTUNvuPazMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFI5JMzMLJVDwszMUjkkzMwslUPCzMxSOSTMzCyVQ8LMzFLlFBKS9pH0jKTFyeveKe06S5oh6e+SFkk6Ppk/QdI7kkqSr2G51GNmZvUr1zOJccDsiOgFzE6mK3Mb8HREHAb0ARZlLZsUEUXJ15M51mNmZvUo15AYAUxJ3k8BTq/YQFIn4GTgVwARsSUi1ue4XzMzawS5hsT+EbESIHndr5I2hwBrgHskLZR0l6T2WcsvlVQq6e607ioASWMlzZc0f82aNTmWbWZmNVFtSEiaJem1Sr5G1HAfbYB+wB0R0Rf4iH93S90BHAoUASuBW9I2EhF3RkT/iOjftWvXGu7azMxy0aa6BhFxStoySaskdYuIlZK6AasrabYcWB4R85LpGSQhERGrsrb1S+Dx2hRvZmYNK9fupkeBMcn7McAjFRtExLvAMkm9k1mDgL8BJMGyw1eA13Ksx8zM6lGuITERGCxpMTA4mUbSAZKyP6n0LWCapFIyXUs3JPNvkvRqMr8YuCLHepqFN9e9ySVPXEKnGzvR6tpWdLqxE5c8cQlvrnsz36WZme1EEZHvGmqtf//+MX/+/HyXUSdPLX6KMx88k7JtZZRtLyufX9CqgILWBcw4awZDew3NY4Vm1lJJWhAR/Wuzju+4bkRvrnuTMx88k4/LPt4pIADKtpfxcdnHnPngmT6jMLMmwyHRiG554RbKtpVV2aZsWxmT/jKpkSoyM6uaQ6IRTS2dussZREVl28u4r/S+RqrIzKxqDolGtHHLxnptZ2bW0BwSjajDHh3qtZ2ZWUNzSDSi8wrPo6BVQZVtCloVMLpwdCNVZGZWNYdEI7ry+CspaF1NSLQu4IoBu8XtImbWDDgkGtGh+xzKjLNmsFfBXrucURS0KmCvgr2YcdYMDt3n0DxVaGa2M4dEIxvaayilF5cy9uixdGrbiVZqRae2nRh79FhKLy71jXRm1qT4jmszs92E77g2M7N65ZAwM7NUDgkzM0vVLK9JSFoDLG2EXXUB3muE/TSGlnQs0LKOpyUdC7Ss42lJxwLQOyI61maFap9M1xRFRKM8v1TS/Npe5GmqWtKxQMs6npZ0LNCyjqclHQtkjqe267i7yczMUjkkzMwslUOianfmu4B61JKOBVrW8bSkY4GWdTwt6VigDsfTLC9cm5lZ4/CZhJmZpXJImJlZKodEJSR9SdLrkt6QNC7f9eRC0kGS5khaJOmvkr6d75pyJam1pIWSHs93LbmS1FnSDEl/T35Gx+e7prqSdEXyb+w1SfdLapfvmmpD0t2SVkt6LWvePpKekbQ4ed07nzXWVMqx3Jz8OyuV9LCkzjXZlkOiAkmtgV8AQ4EjgHMlHZHfqnKyFbgyIg4HBgD/1cyPB+DbwKJ8F1FPbgOejojDgD400+OSdCBwGdA/Io4EWgPn5LeqWrsX+FKFeeOA2RHRC5idTDcH97LrsTwDHBkRhcA/gKtrsiGHxK6OBd6IiH9GxBbgAWBEnmuqs4hYGREvJ+83kPkldGB+q6o7Sd2BLwN35buWXEnqBJwM/AogIrZExPq8FpWbNsCektoAewEr8lxPrUTEs8C6CrNHAFOS91OA0xuzprqq7FgiYmZEbE0m/wJ0r8m2HBK7OhBYljW9nGb8SzWbpJ5AX2BenkvJxa3Ad4Htea6jPhwCrAHuSbrP7pLUPt9F1UVEvAP8FHgbWAl8EBEz81tVvdg/IlZC5g8uYL8811NfLgSeqklDh8SuVMm8Zv85YUkdgN8Bl0fEh/mupy4knQqsjogF+a6lnrQB+gF3RERf4COaT3fGTpK++hHAwcABQHtJ5+W3KquMpB+Q6YaeVpP2DoldLQcOypruTjM7ba5IUgGZgJgWEQ/lu54cnAAMl7SETDfgFyRNzW9JOVkOLI+IHWd2M8iERnN0CvBWRKyJiDLgIWBgnmuqD6skdQNIXlfnuZ6cSBoDnAqMihreJOeQ2NVLQC9JB0vag8zFt0fzXFOdSRKZPu9FEfGzfNeTi4i4OiK6R0RPMj+XP0REs/1rNSLeBZZJ6p3MGgT8LY8l5eJtYICkvZJ/c4NophfhK3gUGJO8HwM8ksdaciLpS8D3gOER8XFN13NIVJBc2LkU+D8y/8h/GxF/zW9VOTkBGE3mr+6S5GtYvouyct8CpkkqBYqAG/JbTt0kZ0MzgJeBV8n8bmlWQ1pIuh94AegtabmkrwMTgcGSFgODk+kmL+VYfg50BJ5Jfg9MrtG2PCyHmZml8ZmEmZmlckiYmVkqh4SZmaVySJiZWSqHhJmZpXJI2G5D0r5ZHwN+V9I7yfuNkv5fA+3zB8nIqKXJvo5L5l8uaa+G2KdZffJHYG23JGkCsDEiftqA+zge+Bnw+Yj4l6QuwB4RsSK5a7x/RLzXUPs3qw8+k7DdnqTP73g2haQJkqZImilpiaSvSrpJ0quSnk6GOEHS0ZL+KGmBpP/bMXRDBd2A9yLiXwAR8V4SEJeRGd9ojqQ5yfaGSHpB0suSHkzG2iKp4X8kvZh8fboxvidmOzgkzHZ1KJnhyEcAU4E5EXEUsAn4chIU/wucGRFHA3cDP6lkOzOBgyT9Q9L/k/Q5gIi4ncx4YMURUZycYfwQOCUi+gHzge9kbefDiDiWzB2zt9b/4Zqla5PvAsyaoKciokzSq2QenvN0Mv9VoCfQGziSzPAGJG1WVtxIRGyUdDRwElAMTJc0LiLurdB0AJkHXP0p2d4eZIZU2OH+rNdJuR6cWW04JMx2taN7aLuksqzRMreT+T8j4K8RsdOjRiUdBDyWTE6OiMkRsQ2YC8xNQmcMmaeG7bQq8ExEnJtST6S8N2tw7m4yq73Xga7JhWkkFUj6bEQsi4ii5GuypN6SemWtVwQsTd5vIDPYGmSeEnbCjusNyUiqn8lab2TWa/YZhlmD85mEWS1FxBZJZwK3S/oEmf9HtwIVRwvuAPxv8sD5rcAbwNhk2Z3AU5JWJtclLgDul9Q2Wf5DMs8hBmgraR6ZP+rSzjbMGoQ/AmvWhPmjspZv7m4yM7NUPpMwM7NUPpMwM7NUDgkzM0vlkDAzs1QOCTMzS+WQMDOzVP8fORimTPPulhAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "for x, y in dataset_val.take(1):\n",
    "    show_plot(\n",
    "        [x[0][:, 0].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        1,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.42181746 -0.80767213 -0.95051313 ...  0.37707484  0.36564732\n",
      "    0.36564732]]\n",
      "\n",
      " [[-0.42181746 -0.80767213 -0.95051313 ... -0.67448177 -0.77965774\n",
      "   -0.77965774]]\n",
      "\n",
      " [[-0.42181746 -0.80767213 -0.95051313 ... -0.76592148 -0.77965774\n",
      "   -0.77965774]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.93802911  0.82816764 -0.49254006 ... -0.81164133 -0.77965774\n",
      "   -0.77965774]]\n",
      "\n",
      " [[ 0.93802911  0.82816764 -0.49254006 ... -0.24014317 -0.20700521\n",
      "   -0.20700521]]\n",
      "\n",
      " [[ 0.93802911  0.82816764 -0.49254006 ... -0.73163159 -0.77965774\n",
      "   -0.77965774]]], shape=(256, 1, 9), dtype=float64)\n",
      "predicted denormalized: [[3184.4968]]\n",
      "   actual denormalized: [3721.]\n",
      "             predicted: [[-0.6611118]]\n",
      "                actual: [-0.43428471]\n"
     ]
    }
   ],
   "source": [
    "def denormalize(value):\n",
    "    data_mean = mean[9]\n",
    "    data_std = std[9]\n",
    "    return value*data_std+data_mean\n",
    "\n",
    "for x, y in dataset_val.take(1):\n",
    "    predictionData = model.predict(x)\n",
    "    denormalized_predictionData = denormalize(predictionData)[0]\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    actualValue = y[0].numpy()\n",
    "    print(\"predicted denormalized:\", denormalized_predictionData)\n",
    "    print(\"   actual denormalized:\", denormalize(actualValue))\n",
    "    print(\"             predicted:\", predictionData[0])\n",
    "    print(\"                actual:\", actualValue)\n",
    "\n",
    "# for x, y in datasetPredTest.take(1):\n",
    "#     show_plot(\n",
    "#         [x[0][:, 0].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "#         1,\n",
    "#         \"Single Step Prediction\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted denormalized: [[[3404.664]]]\n",
      "             predicted: [[[-0.47361678]]]\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    return (data - mean) / std\n",
    "\n",
    "predTest = pd.read_csv('TestData/testData1.csv',';')\n",
    "mean = train[titles].values.mean(axis=0)\n",
    "std = train[titles].values.std(axis=0)\n",
    "featuresPredTest = predTest[titles]\n",
    "predTestRange = int(featuresPredTest.shape[0])\n",
    "featuresPredTest = normalize(featuresPredTest.values)\n",
    "featuresPredTest = pd.DataFrame(featuresPredTest)\n",
    "x_predTest = featuresPredTest[[i for i in range(9)]].values\n",
    "y_predTest = featuresPredTest.iloc[0:][[9]]\n",
    "\n",
    "datasetPredTest = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_predTest,\n",
    "    y_predTest,\n",
    "    sequence_length=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "for x, y in datasetPredTest.take(1):\n",
    "    predictionData = model.predict(x)\n",
    "    denormalized_predictionData = denormalize(predictionData)\n",
    "    print(\"predicted denormalized:\", denormalized_predictionData)\n",
    "    print(\"             predicted:\", predictionData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.9686383 ,  -4.7919054 ,  -2.18749335,  -2.03645433,\n",
       "        -10.98795274,  -9.10822432,  -2.533539  ,  -2.5328308 ,\n",
       "         -2.62984099,  -2.03179847],\n",
       "       [ -4.9686383 ,  -4.7919054 ,  -2.18749335,  -2.03645433,\n",
       "        -10.98795274,  -9.10822432,  -2.533539  ,  -2.5328308 ,\n",
       "         -2.62984099,  -2.03179847],\n",
       "       [ -5.0187706 ,  -4.79190541,  -2.28319955,  -1.22926922,\n",
       "        -12.87324877, -11.34995626,  -2.52841539,  -2.5328308 ,\n",
       "         -2.62984099,  -2.03179847]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x,y in datasetPredTest.take(2):\n",
    "#     print(x[0][0].numpy(\n",
    "normalize(featuresPredTest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\weick\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\weick\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: kerasModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"kerasModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4440.145013048905 2186.3270118261803\n"
     ]
    }
   ],
   "source": [
    "print(mean[9], std[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.21256498, -2.87541672, -0.77877323, ..., -0.85736119,\n",
       "        -0.77965774, -0.77965774],\n",
       "       [-3.21256498, -2.87541672, -0.77877323, ..., -0.85736119,\n",
       "        -0.77965774, -0.77965774],\n",
       "       [-3.21256498, -2.87541672, -0.77877323, ..., -0.22871321,\n",
       "        -0.20700521, -0.20700521],\n",
       "       ...,\n",
       "       [-0.45169101, -0.769843  , -0.89326649, ..., -0.73163159,\n",
       "        -0.77965774, -0.77965774],\n",
       "       [-0.45169101, -0.769843  , -0.89326649, ...,  2.2515888 ,\n",
       "         2.08360491,  2.08360491],\n",
       "       [-0.42181746, -0.80767213, -0.95051313, ..., -0.24014317,\n",
       "        -0.20700521, -0.20700521]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12338, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.243078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.362727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.851154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.057052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.138227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>-0.438090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>-1.175009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>-0.977567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>0.471750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>-0.682039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12338 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              9\n",
       "0     -1.243078\n",
       "1     -1.362727\n",
       "2     -0.851154\n",
       "3     -1.057052\n",
       "4     -1.138227\n",
       "...         ...\n",
       "12333 -0.438090\n",
       "12334 -1.175009\n",
       "12335 -0.977567\n",
       "12336  0.471750\n",
       "12337 -0.682039\n",
       "\n",
       "[12338 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12338, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.49957229e+01, 7.58211654e+08, 3.51591966e+01, 3.95126518e+00,\n",
       "       8.80759900e+00, 1.03141433e+01, 2.20348746e+02, 2.18379099e+01,\n",
       "       4.36758198e+00, 4.44014501e+03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78602711e+01, 1.58227593e+08, 1.61621817e+01, 1.92785362e+00,\n",
       "       1.01701540e+00, 8.50091488e-01, 8.72457286e+01, 8.70529350e+00,\n",
       "       1.74105870e+00, 2.18632701e+03])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.49957229e+01, 7.58211654e+08, 3.51591966e+01, 3.95126518e+00,\n",
       "       8.80759900e+00, 1.03141433e+01, 2.20348746e+02, 2.18379099e+01,\n",
       "       4.36758198e+00, 4.44014501e+03])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[titles].values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[[3.96250010e+00, 5.92916600e+07, 7.00000000e+00, 7.00000000e+00, 4.53000021e+00, 1.08999996e+01, 1.99000000e+02, 2.00000000e+01, 4.00000000e+00, 0.00000000e+00]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3.9625001,\n",
       "   59291660.0,\n",
       "   7.0,\n",
       "   7.0,\n",
       "   4.53000021,\n",
       "   10.8999996,\n",
       "   199.0,\n",
       "   20.0,\n",
       "   4.0,\n",
       "   0.0]]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.53706566, -4.41718148, -1.74228932,  1.58141406,\n",
       "         -4.20603149,  0.68916852, -0.24469675, -0.21112555,\n",
       "         -0.21112555, -2.03086958]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test-train[titles].values.mean(axis=0))/train[titles].values.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.49957229e+01, 7.58211654e+08, 3.51591966e+01, 3.95126518e+00,\n",
       "       8.80759900e+00, 1.03141433e+01, 2.20348746e+02, 2.18379099e+01,\n",
       "       4.36758198e+00, 4.44014501e+03])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[titles].values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5313690e+01, 2.8448510e+08, 2.4000000e+01, ..., 1.5000000e+01,\n",
       "        3.0000000e+00, 1.8080000e+03],\n",
       "       [2.5313690e+01, 2.8448510e+08, 2.4000000e+01, ..., 1.5000000e+01,\n",
       "        3.0000000e+00, 1.5250000e+03],\n",
       "       [2.5313690e+01, 2.8448510e+08, 2.4000000e+01, ..., 2.0000000e+01,\n",
       "        4.0000000e+00, 2.7350000e+03],\n",
       "       ...,\n",
       "       [1.0126033e+02, 8.9728690e+08, 1.9000000e+01, ..., 2.0000000e+01,\n",
       "        4.0000000e+00, 2.0260000e+03],\n",
       "       [1.0126033e+02, 8.9728690e+08, 1.9000000e+01, ..., 2.5000000e+01,\n",
       "        5.0000000e+00, 2.5550000e+03],\n",
       "       [1.0178664e+02, 9.0085286e+08, 2.0000000e+01, ..., 1.5000000e+01,\n",
       "        3.0000000e+00, 1.8390000e+03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[titles].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
