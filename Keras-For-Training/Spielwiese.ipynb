{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assembly</th>\n",
       "      <th>Material</th>\n",
       "      <th>OpenOrders</th>\n",
       "      <th>NewOrders</th>\n",
       "      <th>TotalWork</th>\n",
       "      <th>TotalSetup</th>\n",
       "      <th>SumDuration</th>\n",
       "      <th>SumOperations</th>\n",
       "      <th>ProductionOrders</th>\n",
       "      <th>CycleTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>68552.000000</td>\n",
       "      <td>6.855200e+04</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "      <td>68552.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>83.834927</td>\n",
       "      <td>7.140528e+08</td>\n",
       "      <td>17.922424</td>\n",
       "      <td>2.417129</td>\n",
       "      <td>5.589963</td>\n",
       "      <td>9.591012</td>\n",
       "      <td>221.157340</td>\n",
       "      <td>21.914167</td>\n",
       "      <td>4.382833</td>\n",
       "      <td>3196.616802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.152932</td>\n",
       "      <td>1.803094e+08</td>\n",
       "      <td>13.365124</td>\n",
       "      <td>1.801401</td>\n",
       "      <td>2.404391</td>\n",
       "      <td>2.295356</td>\n",
       "      <td>87.725997</td>\n",
       "      <td>8.740231</td>\n",
       "      <td>1.748046</td>\n",
       "      <td>1653.565104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.024405</td>\n",
       "      <td>6.544642e+07</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>73.395460</td>\n",
       "      <td>6.259937e+08</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>8.860000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2053.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>92.351915</td>\n",
       "      <td>7.821824e+08</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.240000</td>\n",
       "      <td>10.180000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>99.191800</td>\n",
       "      <td>8.479752e+08</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.440000</td>\n",
       "      <td>11.120000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3778.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>108.421820</td>\n",
       "      <td>9.245294e+08</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.480000</td>\n",
       "      <td>13.530000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14855.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Assembly      Material    OpenOrders     NewOrders     TotalWork  \\\n",
       "count  68552.000000  6.855200e+04  68552.000000  68552.000000  68552.000000   \n",
       "mean      83.834927  7.140528e+08     17.922424      2.417129      5.589963   \n",
       "std       21.152932  1.803094e+08     13.365124      1.801401      2.404391   \n",
       "min        8.024405  6.544642e+07      2.000000      0.000000      0.000000   \n",
       "25%       73.395460  6.259937e+08     10.000000      1.000000      3.880000   \n",
       "50%       92.351915  7.821824e+08     13.000000      2.000000      5.240000   \n",
       "75%       99.191800  8.479752e+08     19.000000      3.000000      7.440000   \n",
       "max      108.421820  9.245294e+08     91.000000     12.000000     11.480000   \n",
       "\n",
       "         TotalSetup   SumDuration  SumOperations  ProductionOrders  \\\n",
       "count  68552.000000  68552.000000   68552.000000      68552.000000   \n",
       "mean       9.591012    221.157340      21.914167          4.382833   \n",
       "std        2.295356     87.725997       8.740231          1.748046   \n",
       "min        0.000000    145.000000      15.000000          3.000000   \n",
       "25%        8.860000    153.000000      15.000000          3.000000   \n",
       "50%       10.180000    197.000000      20.000000          4.000000   \n",
       "75%       11.120000    255.000000      25.000000          5.000000   \n",
       "max       13.530000    442.000000      45.000000          9.000000   \n",
       "\n",
       "          CycleTime  \n",
       "count  68552.000000  \n",
       "mean    3196.616802  \n",
       "std     1653.565104  \n",
       "min     1460.000000  \n",
       "25%     2053.000000  \n",
       "50%     2660.000000  \n",
       "75%     3778.000000  \n",
       "max    14855.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import sklearn\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "#from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "#set_random_seed(1)\n",
    "seed(1)\n",
    "\n",
    "train = pd.read_csv('TestData/full_trainset.csv',',')\n",
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assembly</th>\n",
       "      <th>Material</th>\n",
       "      <th>OpenOrders</th>\n",
       "      <th>NewOrders</th>\n",
       "      <th>TotalWork</th>\n",
       "      <th>TotalSetup</th>\n",
       "      <th>SumDuration</th>\n",
       "      <th>SumOperations</th>\n",
       "      <th>ProductionOrders</th>\n",
       "      <th>CycleTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.898176</td>\n",
       "      <td>93773420</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.32</td>\n",
       "      <td>158</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.898176</td>\n",
       "      <td>93773420</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.32</td>\n",
       "      <td>245</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>3027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.898176</td>\n",
       "      <td>93773420</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.32</td>\n",
       "      <td>150</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.310417</td>\n",
       "      <td>129137490</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.16</td>\n",
       "      <td>7.93</td>\n",
       "      <td>201</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.310417</td>\n",
       "      <td>129137490</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.16</td>\n",
       "      <td>7.93</td>\n",
       "      <td>152</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68547</th>\n",
       "      <td>105.828920</td>\n",
       "      <td>882328600</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3.28</td>\n",
       "      <td>8.08</td>\n",
       "      <td>151</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68548</th>\n",
       "      <td>105.553474</td>\n",
       "      <td>893276000</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5.45</td>\n",
       "      <td>11.52</td>\n",
       "      <td>156</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68549</th>\n",
       "      <td>105.553474</td>\n",
       "      <td>893276000</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5.45</td>\n",
       "      <td>11.52</td>\n",
       "      <td>255</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>3365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68550</th>\n",
       "      <td>106.431946</td>\n",
       "      <td>897071550</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>11.95</td>\n",
       "      <td>158</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68551</th>\n",
       "      <td>106.431946</td>\n",
       "      <td>897071550</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>11.95</td>\n",
       "      <td>151</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68552 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Assembly   Material  OpenOrders  NewOrders  TotalWork  TotalSetup  \\\n",
       "0       16.898176   93773420           6          0       0.66        1.32   \n",
       "1       16.898176   93773420           6          0       0.66        1.32   \n",
       "2       16.898176   93773420           6          0       0.66        1.32   \n",
       "3       16.310417  129137490           8          0       3.16        7.93   \n",
       "4       16.310417  129137490           8          0       3.16        7.93   \n",
       "...           ...        ...         ...        ...        ...         ...   \n",
       "68547  105.828920  882328600          12          4       3.28        8.08   \n",
       "68548  105.553474  893276000          14          5       5.45       11.52   \n",
       "68549  105.553474  893276000          14          5       5.45       11.52   \n",
       "68550  106.431946  897071550          15          0       6.40       11.95   \n",
       "68551  106.431946  897071550          15          0       6.40       11.95   \n",
       "\n",
       "       SumDuration  SumOperations  ProductionOrders  CycleTime  \n",
       "0              158             15                 3       2074  \n",
       "1              245             25                 5       3027  \n",
       "2              150             15                 3       1906  \n",
       "3              201             20                 4       2496  \n",
       "4              152             15                 3       2000  \n",
       "...            ...            ...               ...        ...  \n",
       "68547          151             15                 3       1543  \n",
       "68548          156             15                 3       1975  \n",
       "68549          255             25                 5       3365  \n",
       "68550          158             15                 3       1824  \n",
       "68551          151             15                 3       1756  \n",
       "\n",
       "[68552 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.70\n",
    "train_split = int(split_fraction * int(train.shape[0]))\n",
    "step = 1\n",
    "\n",
    "#past = 0\n",
    "#future = 8\n",
    "learning_rate = 0.0005\n",
    "batch = 128\n",
    "epochs = 100\n",
    "mean = 0\n",
    "std = 0\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    global mean\n",
    "    global std\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    mean = data_mean\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    std = data_std\n",
    "    return (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.233757</td>\n",
       "      <td>-3.517613</td>\n",
       "      <td>-2.141174</td>\n",
       "      <td>-3.915902</td>\n",
       "      <td>-0.718292</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.726239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.233757</td>\n",
       "      <td>-3.517613</td>\n",
       "      <td>-2.141174</td>\n",
       "      <td>-3.915902</td>\n",
       "      <td>0.274358</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>-0.196263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.233757</td>\n",
       "      <td>-3.517613</td>\n",
       "      <td>-2.141174</td>\n",
       "      <td>-3.915902</td>\n",
       "      <td>-0.809571</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.819666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.262115</td>\n",
       "      <td>-3.319046</td>\n",
       "      <td>-1.150467</td>\n",
       "      <td>-0.791841</td>\n",
       "      <td>-0.227672</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.491559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.262115</td>\n",
       "      <td>-3.319046</td>\n",
       "      <td>-1.150467</td>\n",
       "      <td>-0.791841</td>\n",
       "      <td>-0.786751</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.767391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -3.233757 -3.517613 -2.141174 -3.915902 -0.718292 -0.789559 -0.789559   \n",
       "1 -3.233757 -3.517613 -2.141174 -3.915902  0.274358  0.355623  0.355623   \n",
       "2 -3.233757 -3.517613 -2.141174 -3.915902 -0.809571 -0.789559 -0.789559   \n",
       "3 -3.262115 -3.319046 -1.150467 -0.791841 -0.227672 -0.216968 -0.216968   \n",
       "4 -3.262115 -3.319046 -1.150467 -0.791841 -0.786751 -0.789559 -0.789559   \n",
       "\n",
       "          7  \n",
       "0 -0.726239  \n",
       "1 -0.196263  \n",
       "2 -0.819666  \n",
       "3 -0.491559  \n",
       "4 -0.767391  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = ['Assembly',\n",
    "          'Material',\n",
    "#           'OpenOrders',\n",
    "#           'NewOrders',\n",
    "          'TotalWork',\n",
    "          'TotalSetup',\n",
    "          'SumDuration',\n",
    "          'SumOperations',\n",
    "          'ProductionOrders',\n",
    "          'CycleTime']\n",
    "#  \n",
    "# for c in train.columns:\n",
    "#     titles.append(c);\n",
    "    \n",
    "features = train[titles]\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.233757</td>\n",
       "      <td>-3.517613</td>\n",
       "      <td>-2.141174</td>\n",
       "      <td>-3.915902</td>\n",
       "      <td>-0.718292</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.726239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.233757</td>\n",
       "      <td>-3.517613</td>\n",
       "      <td>-2.141174</td>\n",
       "      <td>-3.915902</td>\n",
       "      <td>0.274358</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>-0.196263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.233757</td>\n",
       "      <td>-3.517613</td>\n",
       "      <td>-2.141174</td>\n",
       "      <td>-3.915902</td>\n",
       "      <td>-0.809571</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.819666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.262115</td>\n",
       "      <td>-3.319046</td>\n",
       "      <td>-1.150467</td>\n",
       "      <td>-0.791841</td>\n",
       "      <td>-0.227672</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.491559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.262115</td>\n",
       "      <td>-3.319046</td>\n",
       "      <td>-1.150467</td>\n",
       "      <td>-0.791841</td>\n",
       "      <td>-0.786751</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.767391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47981</th>\n",
       "      <td>1.004468</td>\n",
       "      <td>0.832185</td>\n",
       "      <td>-0.647188</td>\n",
       "      <td>0.177044</td>\n",
       "      <td>-0.684063</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.710112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47982</th>\n",
       "      <td>0.990605</td>\n",
       "      <td>0.811657</td>\n",
       "      <td>-0.877032</td>\n",
       "      <td>-1.321182</td>\n",
       "      <td>0.365636</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>-0.276343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47983</th>\n",
       "      <td>0.972068</td>\n",
       "      <td>0.797372</td>\n",
       "      <td>-0.833441</td>\n",
       "      <td>-0.106532</td>\n",
       "      <td>-0.718292</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.586655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47984</th>\n",
       "      <td>0.959090</td>\n",
       "      <td>0.747386</td>\n",
       "      <td>-1.907367</td>\n",
       "      <td>-2.767420</td>\n",
       "      <td>-0.820980</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.789559</td>\n",
       "      <td>-0.790748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47985</th>\n",
       "      <td>0.959090</td>\n",
       "      <td>0.747386</td>\n",
       "      <td>-1.907367</td>\n",
       "      <td>-2.767420</td>\n",
       "      <td>-0.227672</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.462641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47986 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -3.233757 -3.517613 -2.141174 -3.915902 -0.718292 -0.789559 -0.789559   \n",
       "1     -3.233757 -3.517613 -2.141174 -3.915902  0.274358  0.355623  0.355623   \n",
       "2     -3.233757 -3.517613 -2.141174 -3.915902 -0.809571 -0.789559 -0.789559   \n",
       "3     -3.262115 -3.319046 -1.150467 -0.791841 -0.227672 -0.216968 -0.216968   \n",
       "4     -3.262115 -3.319046 -1.150467 -0.791841 -0.786751 -0.789559 -0.789559   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "47981  1.004468  0.832185 -0.647188  0.177044 -0.684063 -0.789559 -0.789559   \n",
       "47982  0.990605  0.811657 -0.877032 -1.321182  0.365636  0.355623  0.355623   \n",
       "47983  0.972068  0.797372 -0.833441 -0.106532 -0.718292 -0.789559 -0.789559   \n",
       "47984  0.959090  0.747386 -1.907367 -2.767420 -0.820980 -0.789559 -0.789559   \n",
       "47985  0.959090  0.747386 -1.907367 -2.767420 -0.227672 -0.216968 -0.216968   \n",
       "\n",
       "              7  \n",
       "0     -0.726239  \n",
       "1     -0.196263  \n",
       "2     -0.819666  \n",
       "3     -0.491559  \n",
       "4     -0.767391  \n",
       "...         ...  \n",
       "47981 -0.710112  \n",
       "47982 -0.276343  \n",
       "47983 -0.586655  \n",
       "47984 -0.790748  \n",
       "47985 -0.462641  \n",
       "\n",
       "[47986 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = features.loc[0 : train_split - 1] #Training Data\n",
    "val_data = features.loc[train_split:] #Validation Data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.23375697, -3.51761314, -2.14117422, -3.91590237, -0.71829237,\n",
       "       -0.78955893, -0.78955893, -0.72623894])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start = past + future\n",
    "start = 0\n",
    "end = start + train_split\n",
    "\n",
    "x_train = train_data[[i for i in range(len(titles))]].values\n",
    "y_train = features.iloc[start:end][[len(titles)-1]]\n",
    "\n",
    "#sequence_length = int(past / step)\n",
    "sequence_length = 1\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (128, 1, 8)\n",
      "Target shape: (128, 1)\n"
     ]
    }
   ],
   "source": [
    "label_start = train_split\n",
    "valRange = int(train.shape[0]) - train_split\n",
    "\n",
    "# x_val = val_data.iloc[[i for i in range(valRange)]].values\n",
    "x_val = val_data[[i for i in range(len(titles))]].values\n",
    "# x_val = val_data.iloc[[i for i in range(49)]].values\n",
    "y_val = features.iloc[label_start:][[len(titles)-1]]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    \n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 8)]            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1, 100)            900       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1)              101       \n",
      "=================================================================\n",
      "Total params: 1,001\n",
      "Trainable params: 1,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "WARNING:tensorflow:From C:\\Users\\weick\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\weick\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: kerasModel\\assets\n"
     ]
    }
   ],
   "source": [
    "# Long Short Term Memory - Model als Methodik mit Adam --> stochastic gradient descent algorithm\n",
    "\n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "dropout1 = keras.layers.Dropout(0.4)(inputs)\n",
    "dense1 = keras.layers.Dense(100, activation='tanh', kernel_constraint=keras.constraints.MaxNorm(max_value=3, axis=0))(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.4)(dense1)\n",
    "outputs = keras.layers.Dense(1, activation='tanh', kernel_constraint=keras.constraints.MaxNorm(max_value=3, axis=0))(dropout2)\n",
    "learning_rate = 0.001\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.99), loss=keras.losses.MeanAbsoluteError(), metrics=keras.metrics.MeanSquaredError())\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "model.save(\"kerasModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/4000\n",
      "362/375 [===========================>..] - ETA: 0s - loss: 0.5700 - mean_squared_error: 0.7623\n",
      "Epoch 00001: val_loss improved from inf to 0.25617, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.5603 - mean_squared_error: 0.7409 - val_loss: 0.2562 - val_mean_squared_error: 0.0933\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 2/4000\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.4447 - mean_squared_error: 0.5195\n",
      "Epoch 00002: val_loss improved from 0.25617 to 0.18833, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4444 - mean_squared_error: 0.5187 - val_loss: 0.1883 - val_mean_squared_error: 0.0542\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 3/4000\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.4267 - mean_squared_error: 0.5271\n",
      "Epoch 00003: val_loss improved from 0.18833 to 0.14236, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4216 - mean_squared_error: 0.5157 - val_loss: 0.1424 - val_mean_squared_error: 0.0344\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 4/4000\n",
      "363/375 [============================>.] - ETA: 0s - loss: 0.4097 - mean_squared_error: 0.5132\n",
      "Epoch 00004: val_loss improved from 0.14236 to 0.12935, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4037 - mean_squared_error: 0.4997 - val_loss: 0.1293 - val_mean_squared_error: 0.0294\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 5/4000\n",
      "366/375 [============================>.] - ETA: 0s - loss: 0.4077 - mean_squared_error: 0.5175\n",
      "Epoch 00005: val_loss improved from 0.12935 to 0.12362, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4030 - mean_squared_error: 0.5070 - val_loss: 0.1236 - val_mean_squared_error: 0.0273\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 6/4000\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.3992 - mean_squared_error: 0.5076\n",
      "Epoch 00006: val_loss improved from 0.12362 to 0.12016, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3952 - mean_squared_error: 0.4986 - val_loss: 0.1202 - val_mean_squared_error: 0.0261\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 7/4000\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.3943 - mean_squared_error: 0.4980\n",
      "Epoch 00007: val_loss improved from 0.12016 to 0.11987, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3930 - mean_squared_error: 0.4949 - val_loss: 0.1199 - val_mean_squared_error: 0.0260\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 8/4000\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.3908 - mean_squared_error: 0.4950\n",
      "Epoch 00008: val_loss improved from 0.11987 to 0.11768, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3875 - mean_squared_error: 0.4874 - val_loss: 0.1177 - val_mean_squared_error: 0.0253\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 9/4000\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.3885 - mean_squared_error: 0.4900\n",
      "Epoch 00009: val_loss did not improve from 0.11768\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3860 - mean_squared_error: 0.4846 - val_loss: 0.1190 - val_mean_squared_error: 0.0257\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 10/4000\n",
      "364/375 [============================>.] - ETA: 0s - loss: 0.3938 - mean_squared_error: 0.5075\n",
      "Epoch 00010: val_loss did not improve from 0.11768\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3885 - mean_squared_error: 0.4951 - val_loss: 0.1178 - val_mean_squared_error: 0.0253\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 11/4000\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.3885 - mean_squared_error: 0.4936\n",
      "Epoch 00011: val_loss improved from 0.11768 to 0.11722, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3856 - mean_squared_error: 0.4870 - val_loss: 0.1172 - val_mean_squared_error: 0.0252\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 12/4000\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3817 - mean_squared_error: 0.4809\n",
      "Epoch 00012: val_loss did not improve from 0.11722\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3817 - mean_squared_error: 0.4809 - val_loss: 0.1184 - val_mean_squared_error: 0.0255\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 13/4000\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.3849 - mean_squared_error: 0.4891\n",
      "Epoch 00013: val_loss did not improve from 0.11722\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3828 - mean_squared_error: 0.4838 - val_loss: 0.1183 - val_mean_squared_error: 0.0255\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 14/4000\n",
      "366/375 [============================>.] - ETA: 0s - loss: 0.3871 - mean_squared_error: 0.4919\n",
      "Epoch 00014: val_loss improved from 0.11722 to 0.11645, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3825 - mean_squared_error: 0.4818 - val_loss: 0.1164 - val_mean_squared_error: 0.0249\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 15/4000\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.3867 - mean_squared_error: 0.4899\n",
      "Epoch 00015: val_loss did not improve from 0.11645\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3827 - mean_squared_error: 0.4811 - val_loss: 0.1168 - val_mean_squared_error: 0.0251\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 16/4000\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.3864 - mean_squared_error: 0.4953\n",
      "Epoch 00016: val_loss improved from 0.11645 to 0.11624, saving model to kerasModel\\simpleModelCheckpoint.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3816 - mean_squared_error: 0.4842 - val_loss: 0.1162 - val_mean_squared_error: 0.0249\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 17/4000\n",
      "362/375 [===========================>..] - ETA: 0s - loss: 0.3860 - mean_squared_error: 0.4934\n",
      "Epoch 00017: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3798 - mean_squared_error: 0.4791 - val_loss: 0.1168 - val_mean_squared_error: 0.0251\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 18/4000\n",
      "364/375 [============================>.] - ETA: 0s - loss: 0.3868 - mean_squared_error: 0.4962\n",
      "Epoch 00018: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3814 - mean_squared_error: 0.4838 - val_loss: 0.1174 - val_mean_squared_error: 0.0253\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 19/4000\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.3809 - mean_squared_error: 0.4803\n",
      "Epoch 00019: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3796 - mean_squared_error: 0.4773 - val_loss: 0.1167 - val_mean_squared_error: 0.0251\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 20/4000\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3807 - mean_squared_error: 0.4807\n",
      "Epoch 00020: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3807 - mean_squared_error: 0.4807 - val_loss: 0.1172 - val_mean_squared_error: 0.0252\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 21/4000\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.3834 - mean_squared_error: 0.4918\n",
      "Epoch 00021: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3786 - mean_squared_error: 0.4806 - val_loss: 0.1163 - val_mean_squared_error: 0.0250\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 22/4000\n",
      "363/375 [============================>.] - ETA: 0s - loss: 0.3845 - mean_squared_error: 0.4903\n",
      "Epoch 00022: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3788 - mean_squared_error: 0.4771 - val_loss: 0.1172 - val_mean_squared_error: 0.0253\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 23/4000\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.3802 - mean_squared_error: 0.4804\n",
      "Epoch 00023: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3779 - mean_squared_error: 0.4751 - val_loss: 0.1178 - val_mean_squared_error: 0.0255\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 24/4000\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3773 - mean_squared_error: 0.4756\n",
      "Epoch 00024: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3773 - mean_squared_error: 0.4756 - val_loss: 0.1183 - val_mean_squared_error: 0.0256\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 25/4000\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.3793 - mean_squared_error: 0.4800\n",
      "Epoch 00025: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3768 - mean_squared_error: 0.4738 - val_loss: 0.1185 - val_mean_squared_error: 0.0256\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 26/4000\n",
      "366/375 [============================>.] - ETA: 0s - loss: 0.3811 - mean_squared_error: 0.4860\n",
      "Epoch 00026: val_loss did not improve from 0.11624\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3768 - mean_squared_error: 0.4763 - val_loss: 0.1183 - val_mean_squared_error: 0.0256\n"
     ]
    }
   ],
   "source": [
    "path_checkpoint = \"kerasModel/simpleModelCheckpoint.h5\"\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 0.0001\n",
    "    if lr > 0.004:\n",
    "        return lr - 0.0002\n",
    "    else:\n",
    "        if lr > 0.0004:\n",
    "            return lr - 0.000001\n",
    "        else:            \n",
    "            return 0.0001\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=10)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback,modelckpt_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_loss(history, \"Training and Validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "for x, y in dataset_val.take(1):\n",
    "    show_plot(\n",
    "        [x[0][:, 0].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        1,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted denormalized: [[3552.2566]]\n",
      "   actual denormalized: [3247.]\n",
      "             predicted: [[0.09583914]]\n",
      "                actual: [-0.07391816]\n",
      "predicted denormalized: [[4670.245]]\n",
      "   actual denormalized: [4037.]\n",
      "             predicted: [[0.71756727]]\n",
      "                actual: [0.36541127]\n",
      "predicted denormalized: [[1951.2517]]\n",
      "   actual denormalized: [2183.]\n",
      "             predicted: [[-0.7945008]]\n",
      "                actual: [-0.6656226]\n",
      "predicted denormalized: [[3080.5005]]\n",
      "   actual denormalized: [3197.]\n",
      "             predicted: [[-0.16651066]]\n",
      "                actual: [-0.10172382]\n",
      "predicted denormalized: [[3149.4536]]\n",
      "   actual denormalized: [3358.]\n",
      "             predicted: [[-0.12816493]]\n",
      "                actual: [-0.01218959]\n",
      "predicted denormalized: [[2177.1558]]\n",
      "   actual denormalized: [2244.]\n",
      "             predicted: [[-0.6688726]]\n",
      "                actual: [-0.6316997]\n",
      "predicted denormalized: [[4969.902]]\n",
      "   actual denormalized: [5346.]\n",
      "             predicted: [[0.8842104]]\n",
      "                actual: [1.09336344]\n",
      "predicted denormalized: [[1987.1449]]\n",
      "   actual denormalized: [1980.]\n",
      "             predicted: [[-0.7745401]]\n",
      "                actual: [-0.77851358]\n",
      "predicted denormalized: [[3430.6672]]\n",
      "   actual denormalized: [3582.]\n",
      "             predicted: [[0.02822163]]\n",
      "                actual: [0.11237976]\n",
      "predicted denormalized: [[1889.273]]\n",
      "   actual denormalized: [1592.]\n",
      "             predicted: [[-0.82896805]]\n",
      "                actual: [-0.9942855]\n"
     ]
    }
   ],
   "source": [
    "def denormalize(value):\n",
    "    data_mean = mean[len(titles)-1]\n",
    "    data_std = std[len(titles)-1]\n",
    "    return value*data_std+data_mean\n",
    "\n",
    "for x, y in dataset_val.take(10):\n",
    "    predictionData = model.predict(x)\n",
    "    denormalized_predictionData = denormalize(predictionData)[0]\n",
    "    actualValue = y[0].numpy()\n",
    "    print(\"predicted denormalized:\", denormalized_predictionData)\n",
    "    print(\"   actual denormalized:\", denormalize(actualValue))\n",
    "    print(\"             predicted:\", predictionData[0])\n",
    "    print(\"                actual:\", actualValue)\n",
    "\n",
    "# for x, y in datasetPredTest.take(1):\n",
    "#     show_plot(\n",
    "#         [x[0][:, 0].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "#         1,\n",
    "#         \"Single Step Prediction\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[8] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-94758b4c1d39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfeaturesPredTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesPredTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfeaturesPredTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesPredTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mx_predTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturesPredTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0my_predTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturesPredTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[8] not in index'"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    return (data - mean) / std\n",
    "\n",
    "predTest = pd.read_csv('TestData/testData1.csv',';')\n",
    "mean = train[titles].values.mean(axis=0)\n",
    "std = train[titles].values.std(axis=0)\n",
    "featuresPredTest = predTest[titles]\n",
    "predTestRange = int(featuresPredTest.shape[0])\n",
    "featuresPredTest = normalize(featuresPredTest.values)\n",
    "featuresPredTest = pd.DataFrame(featuresPredTest)\n",
    "x_predTest = featuresPredTest[[i for i in range(9)]].values\n",
    "y_predTest = featuresPredTest.iloc[0:][[9]]\n",
    "\n",
    "datasetPredTest = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_predTest,\n",
    "    y_predTest,\n",
    "    sequence_length=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "for x, y in datasetPredTest.take(10):\n",
    "    predictionData = model.predict(x)\n",
    "    denormalized_predictionData = denormalize(predictionData)\n",
    "    print(\"predicted denormalized:\", denormalized_predictionData)\n",
    "    print(\"             predicted:\", predictionData)\n",
    "    print(\"                actual:\", y[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in datasetPredTest.take(2):\n",
    "#     print(x[0][0].numpy(\n",
    "normalize(featuresPredTest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean[9], std[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[titles].values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[[3.96250010e+00, 5.92916600e+07, 7.00000000e+00, 7.00000000e+00, 4.53000021e+00, 1.08999996e+01, 1.99000000e+02, 2.00000000e+01, 4.00000000e+00, 0.00000000e+00]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test-train[titles].values.mean(axis=0))/train[titles].values.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[titles].values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[titles].values.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}