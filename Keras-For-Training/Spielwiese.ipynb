{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assembly</th>\n",
       "      <th>Material</th>\n",
       "      <th>OpenOrders</th>\n",
       "      <th>NewOrders</th>\n",
       "      <th>TotalWork</th>\n",
       "      <th>TotalSetup</th>\n",
       "      <th>SumDuration</th>\n",
       "      <th>SumOperations</th>\n",
       "      <th>ProductionOrders</th>\n",
       "      <th>CycleTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17626.000000</td>\n",
       "      <td>1.762600e+04</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.00000</td>\n",
       "      <td>17626.000000</td>\n",
       "      <td>17626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84.995723</td>\n",
       "      <td>7.582117e+08</td>\n",
       "      <td>35.159197</td>\n",
       "      <td>3.951265</td>\n",
       "      <td>8.807599</td>\n",
       "      <td>10.314143</td>\n",
       "      <td>220.348746</td>\n",
       "      <td>21.83791</td>\n",
       "      <td>4.367582</td>\n",
       "      <td>4440.145013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.860778</td>\n",
       "      <td>1.582321e+08</td>\n",
       "      <td>16.162640</td>\n",
       "      <td>1.927908</td>\n",
       "      <td>1.017044</td>\n",
       "      <td>0.850116</td>\n",
       "      <td>87.248204</td>\n",
       "      <td>8.70554</td>\n",
       "      <td>1.741108</td>\n",
       "      <td>2186.389034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.573511</td>\n",
       "      <td>1.921125e+08</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>77.667220</td>\n",
       "      <td>7.000179e+08</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.270000</td>\n",
       "      <td>9.730000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2802.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>92.715866</td>\n",
       "      <td>8.244681e+08</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3790.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>97.449125</td>\n",
       "      <td>8.703534e+08</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.480000</td>\n",
       "      <td>10.850000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5600.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.246370</td>\n",
       "      <td>9.245294e+08</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.480000</td>\n",
       "      <td>13.280000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14855.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Assembly      Material    OpenOrders     NewOrders     TotalWork  \\\n",
       "count  17626.000000  1.762600e+04  17626.000000  17626.000000  17626.000000   \n",
       "mean      84.995723  7.582117e+08     35.159197      3.951265      8.807599   \n",
       "std       17.860778  1.582321e+08     16.162640      1.927908      1.017044   \n",
       "min       20.573511  1.921125e+08      7.000000      0.000000      2.790000   \n",
       "25%       77.667220  7.000179e+08     24.000000      3.000000      8.270000   \n",
       "50%       92.715866  8.244681e+08     30.000000      4.000000      8.950000   \n",
       "75%       97.449125  8.703534e+08     42.000000      5.000000      9.480000   \n",
       "max      104.246370  9.245294e+08     91.000000     12.000000     11.480000   \n",
       "\n",
       "         TotalSetup   SumDuration  SumOperations  ProductionOrders  \\\n",
       "count  17626.000000  17626.000000    17626.00000      17626.000000   \n",
       "mean      10.314143    220.348746       21.83791          4.367582   \n",
       "std        0.850116     87.248204        8.70554          1.741108   \n",
       "min        7.230000    145.000000       15.00000          3.000000   \n",
       "25%        9.730000    153.000000       15.00000          3.000000   \n",
       "50%       10.260000    197.000000       20.00000          4.000000   \n",
       "75%       10.850000    255.000000       25.00000          5.000000   \n",
       "max       13.280000    442.000000       45.00000          9.000000   \n",
       "\n",
       "          CycleTime  \n",
       "count  17626.000000  \n",
       "mean    4440.145013  \n",
       "std     2186.389034  \n",
       "min     1475.000000  \n",
       "25%     2802.000000  \n",
       "50%     3790.500000  \n",
       "75%     5600.750000  \n",
       "max    14855.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import sklearn\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "#from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "#set_random_seed(1)\n",
    "seed(1)\n",
    "\n",
    "train = pd.read_csv('TestData/train.csv',',')\n",
    "predTest = pd.read_csv('TestData/testData.csv',';')\n",
    "#train = pd.read_csv('TestData/TrainingData_8Weeks.csv',';' , parse_dates=['Time'])\n",
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assembly</th>\n",
       "      <th>Material</th>\n",
       "      <th>OpenOrders</th>\n",
       "      <th>NewOrders</th>\n",
       "      <th>TotalWork</th>\n",
       "      <th>TotalSetup</th>\n",
       "      <th>SumDuration</th>\n",
       "      <th>SumOperations</th>\n",
       "      <th>ProductionOrders</th>\n",
       "      <th>CycleTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>145</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>145</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.31369</td>\n",
       "      <td>284485100</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.36</td>\n",
       "      <td>147</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17621</th>\n",
       "      <td>101.64704</td>\n",
       "      <td>888701300</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5.88</td>\n",
       "      <td>11.96</td>\n",
       "      <td>150</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>101.26033</td>\n",
       "      <td>897286900</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.30</td>\n",
       "      <td>11.62</td>\n",
       "      <td>156</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17623</th>\n",
       "      <td>101.26033</td>\n",
       "      <td>897286900</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.30</td>\n",
       "      <td>11.62</td>\n",
       "      <td>197</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17624</th>\n",
       "      <td>101.26033</td>\n",
       "      <td>897286900</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.30</td>\n",
       "      <td>11.62</td>\n",
       "      <td>255</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>101.78664</td>\n",
       "      <td>900852860</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6.41</td>\n",
       "      <td>11.75</td>\n",
       "      <td>156</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17626 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Assembly   Material  OpenOrders  NewOrders  TotalWork  TotalSetup  \\\n",
       "0       25.31369  284485100          24          3       8.50       10.36   \n",
       "1       25.31369  284485100          24          3       8.50       10.36   \n",
       "2       25.31369  284485100          24          3       8.50       10.36   \n",
       "3       25.31369  284485100          24          3       8.50       10.36   \n",
       "4       25.31369  284485100          24          3       8.50       10.36   \n",
       "...          ...        ...         ...        ...        ...         ...   \n",
       "17621  101.64704  888701300          16          2       5.88       11.96   \n",
       "17622  101.26033  897286900          19          4       6.30       11.62   \n",
       "17623  101.26033  897286900          19          4       6.30       11.62   \n",
       "17624  101.26033  897286900          19          4       6.30       11.62   \n",
       "17625  101.78664  900852860          20          4       6.41       11.75   \n",
       "\n",
       "       SumDuration  SumOperations  ProductionOrders  CycleTime  \n",
       "0              145             15                 3       1808  \n",
       "1              145             15                 3       1525  \n",
       "2              200             20                 4       2735  \n",
       "3              200             20                 4       2248  \n",
       "4              147             15                 3       2056  \n",
       "...            ...            ...               ...        ...  \n",
       "17621          150             15                 3       1860  \n",
       "17622          156             15                 3       2284  \n",
       "17623          197             20                 4       2026  \n",
       "17624          255             25                 5       2555  \n",
       "17625          156             15                 3       1839  \n",
       "\n",
       "[17626 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.70\n",
    "train_split = int(split_fraction * int(train.shape[0]))\n",
    "step = 1\n",
    "\n",
    "#past = 0\n",
    "#future = 8\n",
    "learning_rate = 0.0005\n",
    "batch = 256\n",
    "epochs = 100\n",
    "mean = 0\n",
    "std = 0\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    global mean\n",
    "    global std\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    mean = data_mean\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    std = data_std\n",
    "    return (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.857361</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.243078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.857361</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.362727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.228713</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.851154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.228713</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-1.057052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.212565</td>\n",
       "      <td>-2.875417</td>\n",
       "      <td>-0.778773</td>\n",
       "      <td>-0.532953</td>\n",
       "      <td>-0.344685</td>\n",
       "      <td>0.11669</td>\n",
       "      <td>-0.834501</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.138227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4        5         6  \\\n",
       "0 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.857361   \n",
       "1 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.857361   \n",
       "2 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.228713   \n",
       "3 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.228713   \n",
       "4 -3.212565 -2.875417 -0.778773 -0.532953 -0.344685  0.11669 -0.834501   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.779658 -0.779658 -1.243078  \n",
       "1 -0.779658 -0.779658 -1.362727  \n",
       "2 -0.207005 -0.207005 -0.851154  \n",
       "3 -0.207005 -0.207005 -1.057052  \n",
       "4 -0.779658 -0.779658 -1.138227  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "for c in train.columns:\n",
    "    titles.append(c);\n",
    "    \n",
    "features = train[titles]\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>-0.421817</td>\n",
       "      <td>-0.807672</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.331029</td>\n",
       "      <td>0.377075</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>-0.434285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>-0.421817</td>\n",
       "      <td>-0.807672</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.331029</td>\n",
       "      <td>-0.674482</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.019846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>-0.421817</td>\n",
       "      <td>-0.807672</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.331029</td>\n",
       "      <td>-0.765921</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.967420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12341</th>\n",
       "      <td>-0.421817</td>\n",
       "      <td>-0.807672</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.331029</td>\n",
       "      <td>-0.125844</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.635532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>-0.421817</td>\n",
       "      <td>-0.807672</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.331029</td>\n",
       "      <td>-0.125844</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.772938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17621</th>\n",
       "      <td>0.978366</td>\n",
       "      <td>0.848436</td>\n",
       "      <td>-1.236746</td>\n",
       "      <td>-1.054079</td>\n",
       "      <td>-2.929912</td>\n",
       "      <td>2.021928</td>\n",
       "      <td>-0.800211</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.221093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>0.957134</td>\n",
       "      <td>0.901350</td>\n",
       "      <td>-1.065006</td>\n",
       "      <td>-0.011827</td>\n",
       "      <td>-2.515486</td>\n",
       "      <td>1.617065</td>\n",
       "      <td>-0.731632</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.041831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17623</th>\n",
       "      <td>0.957134</td>\n",
       "      <td>0.901350</td>\n",
       "      <td>-1.065006</td>\n",
       "      <td>-0.011827</td>\n",
       "      <td>-2.515486</td>\n",
       "      <td>1.617065</td>\n",
       "      <td>-0.263003</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-0.207005</td>\n",
       "      <td>-1.150910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17624</th>\n",
       "      <td>0.957134</td>\n",
       "      <td>0.901350</td>\n",
       "      <td>-1.065006</td>\n",
       "      <td>-0.011827</td>\n",
       "      <td>-2.515486</td>\n",
       "      <td>1.617065</td>\n",
       "      <td>0.399935</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>-0.927256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>0.986030</td>\n",
       "      <td>0.923328</td>\n",
       "      <td>-1.007760</td>\n",
       "      <td>-0.011827</td>\n",
       "      <td>-2.406946</td>\n",
       "      <td>1.771866</td>\n",
       "      <td>-0.731632</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-0.779658</td>\n",
       "      <td>-1.229972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5288 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "12338 -0.421817 -0.807672 -0.950513 -1.054079  0.267086  0.331029  0.377075   \n",
       "12339 -0.421817 -0.807672 -0.950513 -1.054079  0.267086  0.331029 -0.674482   \n",
       "12340 -0.421817 -0.807672 -0.950513 -1.054079  0.267086  0.331029 -0.765921   \n",
       "12341 -0.421817 -0.807672 -0.950513 -1.054079  0.267086  0.331029 -0.125844   \n",
       "12342 -0.421817 -0.807672 -0.950513 -1.054079  0.267086  0.331029 -0.125844   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "17621  0.978366  0.848436 -1.236746 -1.054079 -2.929912  2.021928 -0.800211   \n",
       "17622  0.957134  0.901350 -1.065006 -0.011827 -2.515486  1.617065 -0.731632   \n",
       "17623  0.957134  0.901350 -1.065006 -0.011827 -2.515486  1.617065 -0.263003   \n",
       "17624  0.957134  0.901350 -1.065006 -0.011827 -2.515486  1.617065  0.399935   \n",
       "17625  0.986030  0.923328 -1.007760 -0.011827 -2.406946  1.771866 -0.731632   \n",
       "\n",
       "              7         8         9  \n",
       "12338  0.365647  0.365647 -0.434285  \n",
       "12339 -0.779658 -0.779658 -1.019846  \n",
       "12340 -0.779658 -0.779658 -0.967420  \n",
       "12341 -0.207005 -0.207005 -0.635532  \n",
       "12342 -0.207005 -0.207005 -0.772938  \n",
       "...         ...       ...       ...  \n",
       "17621 -0.779658 -0.779658 -1.221093  \n",
       "17622 -0.779658 -0.779658 -1.041831  \n",
       "17623 -0.207005 -0.207005 -1.150910  \n",
       "17624  0.365647  0.365647 -0.927256  \n",
       "17625 -0.779658 -0.779658 -1.229972  \n",
       "\n",
       "[5288 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = features.loc[0 : train_split - 1] #Training Data\n",
    "val_data = features.loc[train_split:] #Validation Data\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = past + future\n",
    "start = 0\n",
    "end = start + train_split\n",
    "\n",
    "x_train = train_data[[i for i in range(10)]].values\n",
    "y_train = features.iloc[start:end][[9]]\n",
    "\n",
    "#sequence_length = int(past / step)\n",
    "sequence_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (256, 1, 10)\n",
      "Target shape: (256, 1)\n"
     ]
    }
   ],
   "source": [
    "label_start = train_split\n",
    "valRange = int(train.shape[0]) - train_split\n",
    "\n",
    "x_val = val_data.iloc[[i for i in range(valRange)]].values\n",
    "# x_val = val_data.iloc[[i for i in range(49)]].values\n",
    "y_val = features.iloc[label_start:][[9]]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    \n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 10)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1, 512)            1071104   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1, 1)              513       \n",
      "=================================================================\n",
      "Total params: 3,170,817\n",
      "Trainable params: 3,170,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# Long Short Term Memory - Model als Methodik mit Adam --> stochastic gradient descent algorithm\n",
    "\n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(512, return_sequences=True)(inputs)\n",
    "lstm_out1 = keras.layers.LSTM(512, return_sequences=True)(lstm_out)\n",
    "# lstm_out2 = keras.layers.LSTM(512, return_sequences=True)(lstm_out1)\n",
    "# lstm_out3 = keras.layers.LSTM(512)(lstm_out)\n",
    "outputs = keras.layers.Dense(1)(lstm_out1)\n",
    "learning_rate = 0.001\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#model.save(\"kerasModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0009990000474974514.\n",
      "Epoch 1/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2170\n",
      "Epoch 00001: val_loss improved from inf to 0.01948, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 3s 57ms/step - loss: 0.2170 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.000998000039882958.\n",
      "Epoch 2/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 00002: val_loss improved from 0.01948 to 0.00711, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 0.0193 - val_loss: 0.0071\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0009970000322684646.\n",
      "Epoch 3/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0053\n",
      "Epoch 00003: val_loss improved from 0.00711 to 0.00162, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 0.0053 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0009960000246539713.\n",
      "Epoch 4/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00004: val_loss improved from 0.00162 to 0.00044, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 0.0012 - val_loss: 4.4396e-04\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000995000017039478.\n",
      "Epoch 5/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.8331e-04\n",
      "Epoch 00005: val_loss improved from 0.00044 to 0.00020, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 48ms/step - loss: 3.8331e-04 - val_loss: 2.0131e-04\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0009940000094249845.\n",
      "Epoch 6/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5285e-04\n",
      "Epoch 00006: val_loss improved from 0.00020 to 0.00014, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 2.5285e-04 - val_loss: 1.4052e-04\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0009930000018104912.\n",
      "Epoch 7/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1416e-04\n",
      "Epoch 00007: val_loss improved from 0.00014 to 0.00011, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 2.1416e-04 - val_loss: 1.0709e-04\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009919999941959978.\n",
      "Epoch 8/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0019e-04\n",
      "Epoch 00008: val_loss improved from 0.00011 to 0.00009, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 48ms/step - loss: 2.0019e-04 - val_loss: 8.6930e-05\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0009909999865815044.\n",
      "Epoch 9/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.9769e-04\n",
      "Epoch 00009: val_loss improved from 0.00009 to 0.00008, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.9735e-04 - val_loss: 7.6194e-05\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.000989999978967011.\n",
      "Epoch 10/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9901e-04\n",
      "Epoch 00010: val_loss improved from 0.00008 to 0.00007, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.9901e-04 - val_loss: 7.3022e-05\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0009889999713525177.\n",
      "Epoch 11/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0209e-04\n",
      "Epoch 00011: val_loss improved from 0.00007 to 0.00007, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 48ms/step - loss: 2.0209e-04 - val_loss: 7.1804e-05\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0009879999637380243.\n",
      "Epoch 12/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0437e-04\n",
      "Epoch 00012: val_loss improved from 0.00007 to 0.00007, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 2.0437e-04 - val_loss: 6.8896e-05\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.000986999956123531.\n",
      "Epoch 13/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0330e-04\n",
      "Epoch 00013: val_loss improved from 0.00007 to 0.00006, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 2.0330e-04 - val_loss: 6.2378e-05\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0009859999485090376.\n",
      "Epoch 14/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9870e-04\n",
      "Epoch 00014: val_loss improved from 0.00006 to 0.00005, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.9870e-04 - val_loss: 5.4420e-05\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0009849999408945442.\n",
      "Epoch 15/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9247e-04\n",
      "Epoch 00015: val_loss improved from 0.00005 to 0.00005, saving model to simpleModelCheckpoint.h5\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.9247e-04 - val_loss: 5.1948e-05\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0009839999332800508.\n",
      "Epoch 16/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8610e-04\n",
      "Epoch 00016: val_loss did not improve from 0.00005\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.8610e-04 - val_loss: 5.8489e-05\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0009829999256655575.\n",
      "Epoch 17/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7888e-04\n",
      "Epoch 00017: val_loss did not improve from 0.00005\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.7888e-04 - val_loss: 7.1686e-05\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.000981999918051064.\n",
      "Epoch 18/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6850e-04\n",
      "Epoch 00018: val_loss did not improve from 0.00005\n",
      "49/49 [==============================] - 2s 46ms/step - loss: 1.6850e-04 - val_loss: 8.9941e-05\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0009809999104365707.\n",
      "Epoch 19/4000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.5494e-04\n",
      "Epoch 00019: val_loss did not improve from 0.00005\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.5446e-04 - val_loss: 1.1063e-04\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0009799999028220774.\n",
      "Epoch 20/4000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3959e-04\n",
      "Epoch 00020: val_loss did not improve from 0.00005\n",
      "49/49 [==============================] - 2s 47ms/step - loss: 1.3959e-04 - val_loss: 1.2745e-04\n"
     ]
    }
   ],
   "source": [
    "path_checkpoint = \"simpleModelCheckpoint.h5\"\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "#     return 0.001\n",
    "    if lr > 0.004:\n",
    "        return lr - 0.0002\n",
    "    else:\n",
    "        if lr > 0.0004:\n",
    "            return lr - 0.000001\n",
    "        else:            \n",
    "            return 0.0001\n",
    "#     if epoch < 10:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         if lr < 0.002:\n",
    "#             return lr - 0.00001\n",
    "#         else:\n",
    "#             return lr - 0.0015\n",
    "#     if lr < 0.002:\n",
    "#         return lr - 0.00001\n",
    "#     else:\n",
    "#         return lr\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback,modelckpt_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnyElEQVR4nO3de5gU5Zn+8e/DcBhgOA+MAsohUQgGGHBAA4poTCJqBIlGWVZAjKdoWPWn0V0TITFuNgkxhkTjEs+GLLrJgsQgGoiKSEw4qCgGFHWMExQ5HwSEGZ7fH1WDTVM93T09PT1M35/r6qurq+qtfrpo+p6q6vdtc3dERETiNcl1ASIi0jApIEREJJICQkREIikgREQkkgJCREQiKSBERCSSAkLqhZk9ZWYT63rdXDKzcjM7Mwvbfc7MvhFOjzezZ1JZtxbPc6yZ7TKzgtrWWsO23cw+W9fblfqlgJCEwg+P6tsBM9sT83h8Otty91Hu/nBdr9sQmdm/m9niiPnFZrbPzD6f6rbcfZa7f7mO6jok0Nz9H+5e5O5VdbF9aXwUEJJQ+OFR5O5FwD+Ar8bMm1W9npk1zV2VDdKjwDAz6xU3/2LgNXd/PQc1iaRNASFpM7ORZlZhZjeb2YfAg2bWwcyeNLONZrY1nO4e0yb2tMkkM1tiZtPDdd81s1G1XLeXmS02s51mttDM7jaz3ySoO5UabzezF8PtPWNmxTHLLzGz98xss5ndmmj/uHsF8GfgkrhFE4CHk9URV/MkM1sS8/hLZrbGzLab2S8Bi1n2GTP7c1jfJjObZWbtw2WPAscCfwiPAL9tZj3DU0FNw3W6mtk8M9tiZuvM7PKYbU8zs8fN7JFw36w2s7JE+yDuNbQL220M9993zKxJuOyzZvZ8+Ho2mdlj4Xwzs5+Z2UfhslXpHHlJ3VBASG0dBXQEegBXELyXHgwfHwvsAX5ZQ/uTgLVAMfBj4H4zs1qs+1vgb0AnYBqHfyjHSqXGfwEuBboAzYEbAcysH/CrcPtdw+eL/FAPPRxbi5n1AUqB/0mxjsOEYfV74DsE++JtYHjsKsAPw/o+BxxDsE9w90s49CjwxxFP8T9ARdj+AuA/zeyLMcvPA2YD7YF5qdQc+gXQDugNnEYQlJeGy24HngE6EOzPX4TzvwyMAI4Pn+8iYHOKzyd1xd110y3pDSgHzgynRwL7gMIa1i8FtsY8fg74Rjg9CVgXs6wV4MBR6axL8OFaCbSKWf4b4DcpvqaoGr8T8/ibwIJw+jZgdsyy1uE+ODPBtlsBO4Bh4eM7gCdqua+WhNMTgJdi1jOCD/RvJNjuGODlqH/D8HHPcF82JQiTKqBNzPIfAg+F09OAhTHL+gF7ati3DnwWKAA+AfrFLLsSeC6cfgSYCXSPa38G8CZwMtAk1+//fL3pCEJqa6O7761+YGatzOy/w1MIO4DFQHtL/A2ZD6sn3H13OFmU5rpdgS0x8wDeT1RwijV+GDO9O6amrrHbdvePqeEv2rCm/wUmhEc74wmOKmqzr6rF1+Cxj82si5nNNrN/htv9DcGRRiqq9+XOmHnvAd1iHsfvm0JLfv2pmOBI7L0E2/02QdD9LTxtNTl8bX8mOEK5G9hgZjPNrG2Kr0XqiAJCait+GOD/B/QBTnL3tgSnByDmHHkWfAB0NLNWMfOOqWH9TGr8IHbb4XN2StLmYeDrwJeANsCTGdYRX4Nx6Ov9IcG/y4Bwu/8at82ahm5eT7Av28TMOxb4Z5KaktkE7Cc4nXbYdt39Q3e/3N27EhxZ3GPh12PdfYa7nwicQHCq6aYMa5E0KSCkrrQhOJe+zcw6AlOz/YTu/h6wHJhmZs3N7AvAV7NU4++Ac83sFDNrDnyf5P9/XgC2EZxCme3u+zKs44/ACWY2NvzLfQrBqbZqbYBd4Xa7cfgH6gaC6wCHcff3gaXAD82s0MwGAJcBs6LWT5UHX6F9HLjDzNqYWQ/gBoKjG8zswpgL9FsJQqzKzIaY2Ulm1gz4GNhLcApM6pECQurKXUBLgr8YXwIW1NPzjge+QHC65wfAYwTnvKPcRS1rdPfVwDUEF8U/IPgwq0jSxgnOsfcI7zOqw903ARcC/0Xweo8DXoxZ5XvAYGA7QZj8X9wmfgh8x8y2mdmNEU8xjuC6xHpgDjDV3f+USm1JfIvgQ/4dYAnBPnwgXDYE+KuZ7SK48P1v7v4u0Bb4NcF+fo/g9U6vg1okDRZeEBJpFMKvSa5x96wfwYg0djqCkCNaeCriM2bWxMzOAkYDc3NclkijoB6wcqQ7iuBUSieCUz5Xu/vLuS1JpHHQKSYREYmkU0wiIhKpUZ1iKi4u9p49e+a6DBGRI8aKFSs2uXvnqGWNKiB69uzJ8uXLc12GiMgRw8zeS7RMp5hERCSSAkJERCIpIEREJFKjugYhIvVr//79VFRUsHfv3uQrS04VFhbSvXt3mjVrlnIbBYSI1FpFRQVt2rShZ8+eJP69J8k1d2fz5s1UVFTQq1f8L+EmplNMIlJre/fupVOnTgqHBs7M6NSpU9pHegoIEcmIwuHIUJt/p7wPCHe4/XZ4+ulcVyIi0rDkfUCYwfTpMH9+risRkXRt3ryZ0tJSSktLOeqoo+jWrdvBx/v27aux7fLly5kyZUrS5xg2bFid1Prcc89x7rnn1sm26osuUgMlJbBhQ66rEJF0derUiVdeeQWAadOmUVRUxI03fvpbSJWVlTRtGv0xV1ZWRllZWdLnWLp0aZ3UeiTK+yMIUECINCaTJk3ihhtu4PTTT+fmm2/mb3/7G8OGDWPQoEEMGzaMtWvXAof+RT9t2jQmT57MyJEj6d27NzNmzDi4vaKiooPrjxw5kgsuuIC+ffsyfvx4qkfDnj9/Pn379uWUU05hypQpSY8UtmzZwpgxYxgwYAAnn3wyq1atAuD5558/eAQ0aNAgdu7cyQcffMCIESMoLS3l85//PC+88EKd77NEdARBEBBvvJHrKkSObNddB+Ef83WmtBTuuiv9dm+++SYLFy6koKCAHTt2sHjxYpo2bcrChQv5j//4D37/+98f1mbNmjU8++yz7Ny5kz59+nD11Vcf1mfg5ZdfZvXq1XTt2pXhw4fz4osvUlZWxpVXXsnixYvp1asX48aNS1rf1KlTGTRoEHPnzuXPf/4zEyZM4JVXXmH69OncfffdDB8+nF27dlFYWMjMmTP5yle+wq233kpVVRW7d+9Of4fUkgKCICCeey7XVYhIXbnwwgspKCgAYPv27UycOJG33noLM2P//v2Rbc455xxatGhBixYt6NKlCxs2bKB79+6HrDN06NCD80pLSykvL6eoqIjevXsf7F8wbtw4Zs6cWWN9S5YsORhSZ5xxBps3b2b79u0MHz6cG264gfHjxzN27Fi6d+/OkCFDmDx5Mvv372fMmDGUlpZmsmvSooAAunSBzZth/35Io5OhiMSozV/62dK6deuD09/97nc5/fTTmTNnDuXl5YwcOTKyTYsWLQ5OFxQUUFlZmdI6tfnRtag2ZsYtt9zCOeecw/z58zn55JNZuHAhI0aMYPHixfzxj3/kkksu4aabbmLChAlpP2dt6BoEwREEwMaNua1DROre9u3b6datGwAPPfRQnW+/b9++vPPOO5SXlwPw2GOPJW0zYsQIZs2aBQTXNoqLi2nbti1vv/02/fv35+abb6asrIw1a9bw3nvv0aVLFy6//HIuu+wyVq5cWeevIREdQfBpQGzYAF275rYWEalb3/72t5k4cSJ33nknZ5xxRp1vv2XLltxzzz2cddZZFBcXM3To0KRtpk2bxqWXXsqAAQNo1aoVDz/8MAB33XUXzz77LAUFBfTr149Ro0Yxe/ZsfvKTn9CsWTOKiop45JFH6vw1JNKofpO6rKzMa/ODQUuXwvDhsGABfOUrWShMpJH6+9//zuc+97lcl5Fzu3btoqioCHfnmmuu4bjjjuP666/PdVmHifr3MrMV7h75fV+dYiK4BgH6qquI1M6vf/1rSktLOeGEE9i+fTtXXnllrkuqEzrFxKGnmERE0nX99dc3yCOGTOkIAigqgpYtFRAiIrEUEATjMak3tYjIobIaEGZ2lpmtNbN1ZnZLxPLxZrYqvC01s4Gptq1rXbrARx9l+1lERI4cWQsIMysA7gZGAf2AcWbWL261d4HT3H0AcDswM422dUpHECIih8rmEcRQYJ27v+Pu+4DZwOjYFdx9qbtvDR++BHRPtW1dU0CIHHlGjhzJ03E/5nLXXXfxzW9+s8Y21V+HP/vss9m2bdth60ybNo3p06fX+Nxz587ljZhB3G677TYWLlyYRvXRGtKw4NkMiG7A+zGPK8J5iVwGPJVuWzO7wsyWm9nyjRl0hS4pCXpSHzhQ602ISD0bN24cs2fPPmTe7NmzUxowD4JRWNu3b1+r544PiO9///uceeaZtdpWQ5XNgIj6fbvIXnlmdjpBQNycblt3n+nuZe5e1rlz51oVCkFAVFXBli213oSI1LMLLriAJ598kk8++QSA8vJy1q9fzymnnMLVV19NWVkZJ5xwAlOnTo1s37NnTzZt2gTAHXfcQZ8+fTjzzDMPDgkOQR+HIUOGMHDgQL72ta+xe/duli5dyrx587jpppsoLS3l7bffZtKkSfzud78DYNGiRQwaNIj+/fszefLkg/X17NmTqVOnMnjwYPr378+aNWtqfH25HhY8m/0gKoBjYh53B9bHr2RmA4D7gFHuvjmdtnUptrNccXE2n0mkkcrBeN+dOnVi6NChLFiwgNGjRzN79mwuuugizIw77riDjh07UlVVxRe/+EVWrVrFgAEDIrezYsUKZs+ezcsvv0xlZSWDBw/mxBNPBGDs2LFcfvnlAHznO9/h/vvv51vf+hbnnXce5557LhdccMEh29q7dy+TJk1i0aJFHH/88UyYMIFf/epXXHfddQAUFxezcuVK7rnnHqZPn859992X8PXleljwbB5BLAOOM7NeZtYcuBiYF7uCmR0L/B9wibu/mU7buqbOciJHptjTTLGnlx5//HEGDx7MoEGDWL169SGng+K98MILnH/++bRq1Yq2bdty3nnnHVz2+uuvc+qpp9K/f39mzZrF6tWra6xn7dq19OrVi+OPPx6AiRMnsnjx4oPLx44dC8CJJ554cIC/RJYsWcIll1wCRA8LPmPGDLZt20bTpk0ZMmQIDz74INOmTeO1116jTZs2NW47FVk7gnD3SjO7FngaKAAecPfVZnZVuPxe4DagE3CPmQFUhqeLIttmq1ZQQIhkLEfjfY8ZM4YbbriBlStXsmfPHgYPHsy7777L9OnTWbZsGR06dGDSpEns3bu3xu2En0GHmTRpEnPnzmXgwIE89NBDPJfkx2OSjW9XPWR4oiHFk22rPocFz2o/CHef7+7Hu/tn3P2OcN69YTjg7t9w9w7uXhreympqm03VAaG+ECJHlqKiIkaOHMnkyZMPHj3s2LGD1q1b065dOzZs2MBTTz1V4zZGjBjBnDlz2LNnDzt37uQPf/jDwWU7d+7k6KOPZv/+/QeH6AZo06YNO3fuPGxbffv2pby8nHXr1gHw6KOPctppp9XqteV6WHCNxRRq3x6aNtURhMiRaNy4cYwdO/bgqaaBAwcyaNAgTjjhBHr37s3w4cNrbD948GAuuugiSktL6dGjB6eeeurBZbfffjsnnXQSPXr0oH///gdD4eKLL+byyy9nxowZBy9OAxQWFvLggw9y4YUXUllZyZAhQ7jqqqtq9bpyPSy4hvuO0a0bnHUW3H9/HRYl0ohpuO8ji4b7zoA6y4mIfEoBEUMBISLyKQVEDA3YJ5K+xnSaujGrzb+TAiJG9RGE3u8iqSksLGTz5s0KiQbO3dm8eTOFhYVptdO3mGKUlMAnn8COHdCuXa6rEWn4unfvTkVFBZmMgyb1o7CwkO7duydfMYYCIkZsZzkFhEhyzZo1o1evXrkuQ7JEp5hiqLOciMinFBAxYgfsExHJdwqIGBqPSUTkUwqIGMXFYKaAEBEBBcQhmjYNQkIBISKigDiMOsuJiAQUEHE03IaISEABEUcBISISUEDEUUCIiAQUEHFKSmDXLqiD3/sWETmiKSDiVHeW04VqEcl3Cog46iwnIhJQQMRRQIiIBBQQcTRgn4hIQAERp3Pn4F5HECKS7xQQcQoLg9+CUECISL5TQERQXwgREQVEJAWEiIgCIpIG7BMRUUBE0hGEiIgCIlJJCWzZAvv357oSEZHcUUBEUF8IEREFRCQFhIiIAiJS9YB9ug4hIvlMARFB4zGJiCggIikgREQUEJGKiqBVKwWEiOQ3BUQC6iwnIvkuqwFhZmeZ2VozW2dmt0Qs72tmfzGzT8zsxrhl5Wb2mpm9YmbLs1lnFHWWE5F81zRbGzazAuBu4EtABbDMzOa5+xsxq20BpgBjEmzmdHfflK0aa1JSAuXluXhmEZGGIZtHEEOBde7+jrvvA2YDo2NXcPeP3H0Z0OD6LOsIQkTyXTYDohvwfszjinBeqhx4xsxWmNkViVYysyvMbLmZLd+4cWMtSz1cly6wcSMcOFBnmxQROaJkMyAsYp6n0X64uw8GRgHXmNmIqJXcfaa7l7l7Wefqn4OrAyUlQThs3lxnmxQROaJkMyAqgGNiHncH1qfa2N3Xh/cfAXMITlnVG/WFEJF8l82AWAYcZ2a9zKw5cDEwL5WGZtbazNpUTwNfBl7PWqURFBAiku+y9i0md680s2uBp4EC4AF3X21mV4XL7zWzo4DlQFvggJldB/QDioE5ZlZd42/dfUG2ao2iAftEJN9lLSAA3H0+MD9u3r0x0x8SnHqKtwMYmM3aktGAfSKS79STOoEOHaBZMwWEiOQvBUQCZsFRhAJCRPKVAqIG6iwnIvlMAVEDDdgnIvlMAVEDHUGISD5TQNSgOiA8nf7fIiKNhAKiBiUlsG8fbN+e60pEROqfAqIG1X0hdB1CRPKRAqIGGm5DRPKZAqIGCggRyWcKiBooIEQknykgalBcHPSo1jUIEclHCogaFBQEIaEjCBHJRwqIJNRZTkTylQIiCQWEiOQrBUQSCggRyVcKiCQ0YJ+I5CsFRBIlJbBrF+zenetKRETqlwIiCfWFEJF8pYBIQgEhIvlKAZGEBuwTkXylgEhCRxAikq8UEElUH0EoIEQk3yggkmjRAtq3V0CISP5JKSDMrLWZNQmnjzez88ysWXZLazjUWU5E8lGqRxCLgUIz6wYsAi4FHspWUQ2NOsuJSD5KNSDM3XcDY4FfuPv5QL/sldWw6AhCRPJRygFhZl8AxgN/DOc1zU5JDY8CQkTyUaoBcR3w78Acd19tZr2BZ7NWVQNTUgJbt8K+fbmuRESk/qR0FODuzwPPA4QXqze5+5RsFtaQVH/VdeNG6NYtt7WIiNSXVL/F9Fsza2tmrYE3gLVmdlN2S2s41FlORPJRqqeY+rn7DmAMMB84FrgkW0U1NAoIEclHqQZEs7DfwxjgCXffD3jWqmpgFBAiko9SDYj/BsqB1sBiM+sB7MhWUQ1NdUCoL4SI5JNUL1LPAGbEzHrPzE7PTkkNT+vW0KqVjiBEJL+kepG6nZndaWbLw9tPCY4m8ob6QohIvkn1FNMDwE7g6+FtB/BgtopqiBQQIpJvUg2Iz7j7VHd/J7x9D+idrJGZnWVma81snZndErG8r5n9xcw+MbMb02lb3xQQIpJvUg2IPWZ2SvUDMxsO7KmpgZkVAHcDowjGbRpnZvHjN20BpgDTa9G2XmnAPhHJN6mOp3QV8IiZtQsfbwUmJmkzFFjn7u8AmNlsYDRBRzsA3P0j4CMzOyfdtvWtpCToSV1VBQUFuapCRKT+pHQE4e6vuvtAYAAwwN0HAWckadYNeD/mcUU4LxUptzWzK6ovnm/cuDHFzaevpAQOHIDNm7P2FCIiDUpavyjn7jvCHtUANyRZ3aI2keJTpdzW3We6e5m7l3Xu3DnFzadPneVEJN9k8pOjUR/isSqAY2IedwfWp7jtTNpmRfWAfboOISL5IpOASHY0sAw4zsx6mVlz4GJgXorbzqRtVugIQkTyTY0Xqc1sJ9FBYEDLmtq6e6WZXQs8DRQAD4S/JXFVuPxeMzsKWA60BQ6Y2XWEAwNGtU3vpdUtBYSI5JsaA8Ld22SycXefTzD6a+y8e2OmPyQ4fZRS21xq3x6aN1dAiEj+yOQUU14xC65DKCBEJF8oINKgznIikk8UEGnQcBsikk8UEGlQQIhIPlFApKGkJDjF5HnzW3oiks8UEGno0gX27YPt23NdiYhI9ikg0qC+ECKSTxQQaVBAiEg+UUCkQQEhIvlEAZEGDdgnIvlEAZGG4mJo0kRHECKSHxQQaSgoCEJCASEi+UABkSZ1lhORfKGASJMCQkTyhQIiTRqwT0TyhQIiTTqCEJF8oYBIU0kJfPxxcBMRacwUEGlSZzkRyRcKiDSps5yI5AsFRJp0BCEi+UIBkSYFhIjkCwVEmqpPMSkgRKSxU0CkqXlzaN9eASEijZ8Cohaqf3pURKQxU0DUgjrLiUg+UEDUggJCRPKBAqIWFBAikg8UELXQpQts2wb79uW6EhGR7FFA1EJ1XwhdqBaRxkwBUQvqLCci+UABUQsKCBHJBwqIWtCAfSKSDxQQtaAjCBHJBwqIWmjdOrgpIESkMVNA1JL6QohIY6eAqCUFhIg0dlkNCDM7y8zWmtk6M7slYrmZ2Yxw+SozGxyzrNzMXjOzV8xseTbrrI0uXXSRWkQat6wFhJkVAHcDo4B+wDgz6xe32ijguPB2BfCruOWnu3upu5dlq87a0hGEiDR22TyCGAqsc/d33H0fMBsYHbfOaOARD7wEtDezo7NYU50pKYFNm6CqKteViIhkRzYDohvwfszjinBequs48IyZrTCzKxI9iZldYWbLzWz5xo0b66Ds1JSUwIEDQUiIiDRG2QwIi5jnaawz3N0HE5yGusbMRkQ9ibvPdPcydy/r3Llz7atNkzrLiUhjl82AqACOiXncHVif6jruXn3/ETCH4JRVg6HOciLS2GUzIJYBx5lZLzNrDlwMzItbZx4wIfw208nAdnf/wMxam1kbADNrDXwZeD2LtaZNASEijV3TbG3Y3SvN7FrgaaAAeMDdV5vZVeHye4H5wNnAOmA3cGnYvASYY2bVNf7W3Rdkq9baUECISGOXtYAAcPf5BCEQO+/emGkHrolo9w4wMJu1ZapdO2jeXNcgRKTxUk/qWjILLlTrCEJEGisFRAbUWU5EGjMFRAYUECLSmCkgMqCAEJHGTAGRgeoB+zy++5+ISCOggMhASQns3w/btuW6EhGRuqeAyID6QohIY6aAyIACQkQaMwVEBjRgn4g0ZgqIDOgIQkQaMwVEBjp1giZNFBAi0jgpIDJQUACdOysgRKRxUkBkSOMxiUhjpYDIUEmJLlKLSOOkgMiQhtsQkcZKAZEhBYSINFYKiAyVlMDu3bBrV64rERGpWwqIDKmznIg0VgqIDKmznIg0VgqIDCkgRKSxUkBkSAEhIo2VAgLgu9+Fl16qVdPOnYN7BYSINDYKiC1b4NFH4ZRTYNo0qKxMq3nz5nD88fCzn8GCBdkpUUQkFxQQHTvCq6/Cv/wLfO97QVCsW5fWJp56Co49Fs4+G/7zP/UTpCLSOCggANq1g0cegdmzYe1aKC2F++5L+ZO+d29YuhQuvhhuvRUuuAB27sxuySIi2aaAiHXRRfDaa3DSSXD55XD++bBxY0pNW7eGWbPgpz+FuXPh5JPhrbeyW66ISDYpIOJ17w5/+lPwSf/UU9C/f3CfAjO44Yag+YYNMGQIPPlklusVEckSBUSUJk2CT/ply4KvKZ19Nlx7bTCmRgrOOANWrAhOPX31q/D978OBA1muWUSkjikgajJgQBAS118Pd98NZWWwcmVKTXv0gBdfhEsugalTYexY2LEjy/WKiNQhBUQyhYVw553BeaPt24OLCz/6EVRVJW3asiU8/DD8/OfBqaahQ2HNmnqoWUSkDiggUnXmmcEF7NGj4ZZbgvNI772XtJkZTJkCixYFXS6GDoUnnqiHekVEMqSASEfHjvD448FhwcsvB6egZs1K6euwp50WXJfo0wfGjIHbbtN1CRFp2BQQ6TKDCROCznX9+8O//mvQyW79+qRNjzkGXngBLr0Ubr8dzjsPtm3LfskiIrWhgKitXr3guefgBz+A3/0u+HrssGHB9Ym1axM2KyyE++8Prnk//XRwyumNN+qvbBGRVJk3onEhysrKfPny5fX/xG++GZx6mjs3OI8E0LdvcC5pzJigQ0STw7N4yZKg1/WuXXDddcFpqC98AYqK6rF2EclrZrbC3csilykg6tg//gHz5gVh8dxzwbedjj46uLg9Zgycfnowwl/on/8MTjktWhRckygogEGD4NRTg2GhTjnl01+tExGpawqIXNmyBebPD8JiwQL4+GNo2zboeDd6NIwaFYwDRTB201/+ElyjeOEF+OtfYe/eYDN9+gSBUX3r2TO4FCIikqmcBYSZnQX8HCgA7nP3/4pbbuHys4HdwCR3X5lK2ygNLiBi7dkTHCY88URw27gRmjULvi47ZkzwjagOHYJvSnXowCfenBUrgrBYsiS4VV/Q7tr10MD4/Ocjz2CJiCSVk4AwswLgTeBLQAWwDBjn7m/ErHM28C2CgDgJ+Lm7n5RK2ygNOiBiVVUFP1A0dy7MmQNvv334Oq1aBYER3rx9B7Y26cg/dnbgzY86sOr9Dry7vQNb6UBlUQc6dG1Js5ZNKSxqSotWBbRoHdwXFjWlsHUBLds0DW5FwXSrNgW0atuUojZG69bBWa8mTYKb2afTqd6qj2iS3YtIw1JTQDTN4vMOBda5+zthEbOB0UDsh/xo4BEPUuolM2tvZkcDPVNoe+QqKIDhw4Pbj38cfOupvBy2bg1OS23detjN3iun45aVdNy6ldKPP+brsdvbRRCntXAAo5KmHAi/0OZYwlv18sokyzO5r5bsMUmXJ54fOS/DBEv0/Ck2rhMZ1ZBC+6TLs/hXgCXZSVHLU50XNT/qcfw7LvFyD+cdup1E03XxeFvzzvTZu4q6ls2A6Aa8H/O4guAoIdk63VJsC4CZXQFcAXDsscdmVnEumAXfeOrbN/U2+/YF55tiQ2Tv3uDX8KqqDrs/sL+S/bsr2beniv17Pr3fv7eK/XsrqdpbyYEqxw847g7V9zHT7sCBYN4h61Tfw6cdBsN7i3ns4eP4+0PaHeSH3B3yn9Ej/pMnPAqO+ICIWtc9Pm/SErnNtDeSYfOMa0jyAZxs+5k+v3sKAZN4uSdYfnCbHjEvfgOW3h8ph2/HItc9ZDqmnkPaxz+OeL7q7XvcY4Cqorb0oe5lMyCi/jXj30WJ1kmlbTDTfSYwE4JTTOkUeMRq3jz4alOKX29qArQIbyIiqcpmQFQAx8Q87g7EdzdOtE7zFNqKiEgWZfO7L8uA48ysl5k1By4G5sWtMw+YYIGTge3u/kGKbUVEJIuydgTh7pVmdi3wNMFXVR9w99VmdlW4/F5gPsE3mNYRfM310praZqtWERE5nDrKiYjksZq+5qruVSIiEkkBISIikRQQIiISSQEhIiKRGtVFajPbCCT/oehoxcCmOiynrqm+zKi+zKi+zDTk+nq4e+eoBY0qIDJhZssTXclvCFRfZlRfZlRfZhp6fYnoFJOIiERSQIiISCQFxKdm5rqAJFRfZlRfZlRfZhp6fZF0DUJERCLpCEJERCIpIEREJFJeBYSZnWVma81snZndErHczGxGuHyVmQ2u5/qOMbNnzezvZrbazP4tYp2RZrbdzF4Jb7fVc43lZvZa+NyHjYyYy31oZn1i9ssrZrbDzK6LW6de95+ZPWBmH5nZ6zHzOprZn8zsrfC+Q4K2Nb5fs1jfT8xsTfjvN8fM2idoW+N7IYv1TTOzf8b8G56doG2u9t9jMbWVm9krCdpmff9lzN3z4kYwbPjbQG+CHyR6FegXt87ZwFMEv2h3MvDXeq7xaGBwON2G4Jem42scCTyZw/1YDhTXsDyn+zDu3/tDgk5AOdt/wAhgMPB6zLwfA7eE07cAP0pQf43v1yzW92WgaTj9o6j6UnkvZLG+acCNKfz752T/xS3/KXBbrvZfprd8OoIYCqxz93fcfR8wGxgdt85o4BEPvAS0N7Oj66tAd//A3VeG0zuBvxP8PveRJKf7MMYXgbfdvbY96+uEuy8GtsTNHg08HE4/DIyJaJrK+zUr9bn7M+5eGT58ieAXHXMiwf5LRc72XzUzM+DrwP/U9fPWl3wKiG7A+zGPKzj8wzeVdeqFmfUEBgF/jVj8BTN71cyeMrMT6rcyHHjGzFaY2RURyxvKPryYxP8xc7n/AEo8+OVEwvuoHxdvKPtxMsERYZRk74VsujY8BfZAglN0DWH/nQpscPe3EizP5f5LST4FhEXMi/+ObyrrZJ2ZFQG/B65z9x1xi1cSnDYZCPwCmFvP5Q1398HAKOAaMxsRtzzn+9CCn6k9D/jfiMW53n+pagj78VagEpiVYJVk74Vs+RXwGaAU+IDgNE68nO8/YBw1Hz3kav+lLJ8CogI4JuZxd2B9LdbJKjNrRhAOs9z9/+KXu/sOd98VTs8HmplZcX3V5+7rw/uPgDkEh/Kxcr4PCf7DrXT3DfELcr3/QhuqT7uF9x9FrJPT/WhmE4FzgfEenjCPl8J7ISvcfYO7V7n7AeDXCZ431/uvKTAWeCzROrnaf+nIp4BYBhxnZr3CvzAvBubFrTMPmBB+E+dkYHv1qYD6EJ6zvB/4u7vfmWCdo8L1MLOhBP+Gm+upvtZm1qZ6muBi5utxq+V0H4YS/uWWy/0XYx4wMZyeCDwRsU4q79esMLOzgJuB89x9d4J1UnkvZKu+2Gta5yd43pztv9CZwBp3r4hamMv9l5ZcXyWvzxvBN2zeJPh2w63hvKuAq8JpA+4Ol78GlNVzfacQHAavAl4Jb2fH1XgtsJrgWxkvAcPqsb7e4fO+GtbQEPdhK4IP/HYx83K2/wiC6gNgP8FftZcBnYBFwFvhfcdw3a7A/Jrer/VU3zqC8/fV78F74+tL9F6op/oeDd9bqwg+9I9uSPsvnP9Q9XsuZt1633+Z3jTUhoiIRMqnU0wiIpIGBYSIiERSQIiISCQFhIiIRFJAiIhIJAWESBJmVmWHjhJbZyODmlnP2JFARRqSprkuQOQIsMfdS3NdhEh90xGESC2F4/n/yMz+Ft4+G87vYWaLwsHkFpnZseH8kvD3FV4Nb8PCTRWY2a8t+A2QZ8ysZbj+FDN7I9zO7By9TMljCgiR5FrGnWK6KGbZDncfCvwSuCuc90uCIc8HEAx0NyOcPwN43oOBAgcT9KAFOA64291PALYBXwvn3wIMCrdzVXZemkhi6kktkoSZ7XL3ooj55cAZ7v5OOMjih+7eycw2EQz/sD+c/4G7F5vZRqC7u38Ss42ewJ/c/bjw8c1AM3f/gZktAHYRjDg718NBBkXqi44gRDLjCaYTrRPlk5jpKj69NngOwbhWJwIrwhFCReqNAkIkMxfF3P8lnF5KMHoowHhgSTi9CLgawMwKzKxtoo2aWRPgGHd/Fvg20B447ChGJJv0F4lIci3t0B+eX+Du1V91bWFmfyX4Y2tcOG8K8ICZ3QRsBC4N5/8bMNPMLiM4UriaYCTQKAXAb8ysHcEIuT9z92119HpEUqJrECK1FF6DKHP3TbmuRSQbdIpJREQi6QhCREQi6QhCREQiKSBERCSSAkJERCIpIEREJJICQkREIv1/iCAFSWw/NBgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_loss(history, \"Training and Validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "for x, y in dataset_val.take(1):\n",
    "    show_plot(\n",
    "        [x[0][:, 0].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        1,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted denormalized: [[3712.0298]]\n",
      "             predicted: [[-0.4380771]]\n",
      "                actual: [-0.43428471]\n"
     ]
    }
   ],
   "source": [
    "def denormalize(value):\n",
    "    data_mean = mean[9]\n",
    "    data_std = std[9]\n",
    "    return value*data_std+data_mean\n",
    "\n",
    "for x, y in dataset_val.take(1):\n",
    "    predictionData = model.predict(x)\n",
    "    denormalized_predictionData = denormalize(predictionData)[0]\n",
    "    \n",
    "    actualValue = x[0][:, 9].numpy()\n",
    "    print(\"predicted denormalized:\", denormalized_predictionData)\n",
    "    print(\"             predicted:\", predictionData[0])\n",
    "    print(\"                actual:\", actualValue)\n",
    "\n",
    "# for x, y in datasetPredTest.take(1):\n",
    "#     show_plot(\n",
    "#         [x[0][:, 0].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "#         1,\n",
    "#         \"Single Step Prediction\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted denormalized: [[[4159.993]]]\n",
      "             predicted: [[[-0.14033146]]]\n",
      "          actual value: [-0.14352941]\n",
      "predicted denormalized: [[[2316.95]]]\n",
      "             predicted: [[[-0.7301439]]]\n",
      "          actual value: [-0.73236802]\n",
      "predicted denormalized: [[[3241.4858]]]\n",
      "             predicted: [[[-0.43427297]]]\n",
      "          actual value: [-0.43538855]\n",
      "predicted denormalized: [[[2983.6267]]]\n",
      "             predicted: [[[-0.5167933]]]\n",
      "          actual value: [-0.51603383]\n",
      "predicted denormalized: [[[13362.578]]]\n",
      "             predicted: [[[2.804689]]]\n",
      "          actual value: [2.78690273]\n",
      "predicted denormalized: [[[4896.0103]]]\n",
      "             predicted: [[[0.09520946]]]\n",
      "          actual value: [0.10608696]\n",
      "predicted denormalized: [[[2907.6052]]]\n",
      "             predicted: [[[-0.5411218]]]\n",
      "          actual value: [-0.52243425]\n",
      "predicted denormalized: [[[1741.2593]]]\n",
      "             predicted: [[[-0.914377]]]\n",
      "          actual value: [-0.89941897]\n",
      "predicted denormalized: [[[4580.547]]]\n",
      "             predicted: [[[-0.00574531]]]\n",
      "          actual value: [-0.00048003]\n",
      "predicted denormalized: [[[5723.6055]]]\n",
      "             predicted: [[[0.3600573]]]\n",
      "          actual value: [0.35666339]\n"
     ]
    }
   ],
   "source": [
    "predTest = pd.read_csv('TestData/testData.csv',';')\n",
    "\n",
    "featuresPredTest = predTest[titles]\n",
    "predTestRange = int(featuresPredTest.shape[0])\n",
    "featuresPredTest = normalize(featuresPredTest.values, predTestRange)\n",
    "featuresPredTest = pd.DataFrame(featuresPredTest)\n",
    "x_predTest = featuresPredTest.iloc[[i for i in range(predTestRange)]].values\n",
    "y_predTest = featuresPredTest.iloc[0:][[9]]\n",
    "\n",
    "datasetPredTest = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_predTest,\n",
    "    y_predTest,\n",
    "    sequence_length=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "for x, y in datasetPredTest.take(10):\n",
    "    predictionData = model.predict(x)\n",
    "    denormalized_predictionData = denormalize(predictionData)\n",
    "    print(\"predicted denormalized:\", denormalized_predictionData)\n",
    "    print(\"             predicted:\", predictionData)\n",
    "    actualValue = x[0][:, 9].numpy()\n",
    "    print(\"          actual value:\", actualValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in datasetPredTest.take(2):\n",
    "#     print(x[0][0].numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\weick\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\weick\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: kerasModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"kerasModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.09141054e+01 1.87665892e+08 2.04000000e+01 4.20000000e+00\n",
      " 6.90600000e+00 1.13480000e+01 2.25900000e+02 2.25000000e+01\n",
      " 4.50000000e+00 4.59850000e+03] [2.14813232e+00 1.70025508e+07 1.49666295e+00 1.46969385e+00\n",
      " 3.90875939e-01 2.45470976e-01 8.59039580e+01 8.73212460e+00\n",
      " 1.74642492e+00 3.12479511e+03]\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
